{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 建立并训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from RNNCell_diy import diyLSTMCell\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pfile = './concat_image_train_data.pickle'\n",
    "with open(pfile, \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "train_dataset = train_data[\"train_dataset\"]\n",
    "train_dataset.shape = train_dataset.shape + (1,)\n",
    "train_labels = train_data[\"train_labels\"]\n",
    "train_labels_num = train_data[\"train_labels_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cdic = {\"a\":0, \"b\":1, \"c\":2, \"d\":3, \"e\":4, \"f\":5, \"g\":6, \"h\":7, \"i\":8, \"j\":9, \"remain\":10}\n",
    "cdic_r = {}\n",
    "for key, value in cdic.items():\n",
    "    cdic_r[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size_h = 56\n",
    "image_size_w = 112\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "构建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 5, 1, 12) dtype=float32_ref>, <tf.Variable 'bias1:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(5, 5, 12, 32) dtype=float32_ref>, <tf.Variable 'bias3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(4608, 300) dtype=float32_ref>, <tf.Variable 'bias5:0' shape=(300,) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(300, 260) dtype=float32_ref>, <tf.Variable 'bias6:0' shape=(260,) dtype=float32_ref>, <tf.Variable 'Variable_4:0' shape=(260, 11) dtype=float32_ref>, <tf.Variable 'bias_o:0' shape=(11,) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_0/diy_lstm_cell/weights:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_0/diy_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_1/diy_lstm_cell/weights:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_1/diy_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_2/diy_lstm_cell/weights:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_2/diy_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_0/diy_lstm_cell/weights:0' shape=(271, 1040) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_0/diy_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_1/diy_lstm_cell/weights:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_1/diy_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_2/diy_lstm_cell/weights:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_2/diy_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'Variable_5:0' shape=() dtype=int32_ref>]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "kernel_size = 5\n",
    "pooling_size = 2\n",
    "channels_one = 12\n",
    "channels_two = 32\n",
    "hidden1_size = 300\n",
    "hidden2_size = 260\n",
    "dropout = 0.5\n",
    "lam = 0.01\n",
    "max_time_step = 3\n",
    "rnn_num_nodes = 260\n",
    "rnn_num_layers = 3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size_h, image_size_w, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32,shape=(batch_size, max_time_step))\n",
    "    tf_train_labels_num = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    \n",
    "    #variables\n",
    "    layer_weight1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, num_channels, channels_one],stddev=0.1))\n",
    "    layer_biases1 = tf.Variable(tf.zeros([channels_one]), name=\"bias1\")\n",
    "    \n",
    "    layer_weight3 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels_one, channels_two], stddev=0.1))\n",
    "    layer_biases3 = tf.Variable(tf.constant(1.0, shape=[channels_two]), name=\"bias3\")\n",
    "    \n",
    "    neuron_num = ((((image_size_h-pooling_size)//pooling_size+1-kernel_size+1-pooling_size)//pooling_size+1)**2)*channels_two\n",
    "    layer_weight5 = tf.Variable(tf.truncated_normal([neuron_num, hidden1_size], stddev=0.1))\n",
    "    layer_biases5 = tf.Variable(tf.constant(1.0, shape=[hidden1_size]), name=\"bias5\")\n",
    "    \n",
    "    layer_weight6 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "    layer_biases6 = tf.Variable(tf.constant(1.0, shape=[hidden2_size]), name=\"bias6\")\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([rnn_num_nodes, len(cdic)], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([len(cdic)]), name=\"bias_o\")\n",
    "    \n",
    "    # Model\n",
    "    def conv_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer_weight1, [1,1,1,1], padding=\"SAME\")+layer_biases1\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,4,1], [1,2,4,1], padding=\"VALID\"))\n",
    "        conv = tf.nn.conv2d(pooling, layer_weight3, [1,1,1,1], padding=\"VALID\")+layer_biases3\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding=\"VALID\"))\n",
    "        shape = pooling.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling,[shape[0],shape[1]*shape[2]*shape[3]])\n",
    "        hidden1 = tf.nn.dropout(tf.matmul(reshape, layer_weight5)+layer_biases5, dropout)\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, layer_weight6)+layer_biases6)\n",
    "        return hidden2\n",
    "            \n",
    "    def rnn_model1(input_data):\n",
    "        cell_list = []\n",
    "        for i in range(rnn_num_layers):\n",
    "            rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "            rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "            cell_list.append(rnn_cell)\n",
    "        rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        with tf.variable_scope(\"rnn1\"):\n",
    "            rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "                rnn_cells,\n",
    "                input_data,\n",
    "                dtype=tf.float32)\n",
    "        return rnn_outputs, rnn_state\n",
    "    \n",
    "    def rnn_model2(input_data, init_state):\n",
    "        cell_list = []\n",
    "        for i in range(rnn_num_layers):\n",
    "            rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "            rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "            cell_list.append(rnn_cell)\n",
    "        rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        with tf.variable_scope(\"rnn2\"):\n",
    "            rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "                rnn_cells,\n",
    "                input_data,\n",
    "                dtype=tf.float32,\n",
    "                initial_state=init_state,\n",
    "                sequence_length=tf_train_labels_num+1,\n",
    "                swap_memory=True)\n",
    "        return rnn_outputs, rnn_state\n",
    "        \n",
    "    \n",
    "    #loss\n",
    "    conv_out = conv_model(tf_train_dataset)\n",
    "    conv_out = tf.reshape(conv_out,[batch_size,1,-1])\n",
    "    rnn1_output, rnn1_state = rnn_model1(conv_out)\n",
    "    input_data = tf.concat((tf.constant(cdic[\"remain\"],shape=[batch_size,1]), tf_train_labels),1)\n",
    "    input_data = tf.one_hot(input_data,len(cdic))\n",
    "    tf_train_labels_c = tf.concat((tf_train_labels,tf.constant(cdic[\"remain\"],shape=[batch_size,1])),1)\n",
    "    tf_train_labels_c = tf.one_hot(tf_train_labels_c,len(cdic))\n",
    "    rnn2_outputs, _ = rnn_model2(input_data, rnn1_state)\n",
    "    rnn2_outputs = tf.reshape(rnn2_outputs,[-1, rnn_num_nodes])\n",
    "    logits = tf.reshape(tf.matmul(rnn2_outputs, w) + b, [batch_size, max_time_step+1,-1])\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if not(\"bias\" in v.name)])\n",
    "    print(tf_train_labels_c.shape, logits.shape)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c, logits=logits))+lam*regularization_cost\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "      1.0, global_step, 100, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "      zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #model save\n",
    "    tv = tf.trainable_variables()\n",
    "    print(tv)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "第一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 324.175507 learning rate: 1.000000\n",
      "Minibatch perplexity: 14628.99\n",
      "Average loss at step 100: 192.778021 learning rate: 0.950000\n",
      "Minibatch perplexity: 4600.60\n",
      "Average loss at step 200: 39.921231 learning rate: 0.902500\n",
      "Minibatch perplexity: 4534.00\n",
      "Average loss at step 300: 7.766941 learning rate: 0.857375\n",
      "Minibatch perplexity: 814.31\n",
      "Average loss at step 400: 2.767372 learning rate: 0.814506\n",
      "Minibatch perplexity: 957.81\n",
      "Average loss at step 500: 1.949723 learning rate: 0.773781\n",
      "Minibatch perplexity: 640.95\n",
      "Average loss at step 600: 1.799170 learning rate: 0.735092\n",
      "Minibatch perplexity: 908.45\n",
      "Average loss at step 700: 1.765287 learning rate: 0.698337\n",
      "Minibatch perplexity: 631.31\n",
      "Average loss at step 800: 1.753133 learning rate: 0.663420\n",
      "Minibatch perplexity: 713.79\n",
      "Average loss at step 900: 1.746945 learning rate: 0.630249\n",
      "Minibatch perplexity: 725.67\n",
      "Average loss at step 1000: 1.741987 learning rate: 0.598737\n",
      "Minibatch perplexity: 588.57\n",
      "Average loss at step 1100: 1.738422 learning rate: 0.568800\n",
      "Minibatch perplexity: 647.80\n",
      "Average loss at step 1200: 1.735824 learning rate: 0.540360\n",
      "Minibatch perplexity: 690.93\n",
      "Average loss at step 1300: 1.730040 learning rate: 0.513342\n",
      "Minibatch perplexity: 498.73\n",
      "Average loss at step 1400: 1.731753 learning rate: 0.487675\n",
      "Minibatch perplexity: 685.94\n",
      "Average loss at step 1500: 1.739046 learning rate: 0.463291\n",
      "Minibatch perplexity: 952.45\n",
      "Average loss at step 1600: 1.727044 learning rate: 0.440127\n",
      "Minibatch perplexity: 680.11\n",
      "Average loss at step 1700: 1.731713 learning rate: 0.418120\n",
      "Minibatch perplexity: 766.61\n",
      "Average loss at step 1800: 1.718061 learning rate: 0.397214\n",
      "Minibatch perplexity: 758.73\n",
      "Average loss at step 1900: 1.715243 learning rate: 0.377353\n",
      "Minibatch perplexity: 583.47\n",
      "Average loss at step 2000: 1.717764 learning rate: 0.358486\n",
      "Minibatch perplexity: 848.03\n",
      "Average loss at step 2100: 1.719248 learning rate: 0.340562\n",
      "Minibatch perplexity: 1069.33\n",
      "Average loss at step 2200: 1.717872 learning rate: 0.323533\n",
      "Minibatch perplexity: 693.51\n",
      "Average loss at step 2300: 1.713196 learning rate: 0.307357\n",
      "Minibatch perplexity: 661.96\n",
      "Average loss at step 2400: 1.707733 learning rate: 0.291989\n",
      "Minibatch perplexity: 630.33\n",
      "Average loss at step 2500: 1.710368 learning rate: 0.277389\n",
      "Minibatch perplexity: 882.66\n",
      "Average loss at step 2600: 1.711943 learning rate: 0.263520\n",
      "Minibatch perplexity: 751.35\n",
      "Average loss at step 2700: 1.707069 learning rate: 0.250344\n",
      "Minibatch perplexity: 711.61\n",
      "Average loss at step 2800: 1.701438 learning rate: 0.237827\n",
      "Minibatch perplexity: 623.58\n",
      "Average loss at step 2900: 1.705063 learning rate: 0.225935\n",
      "Minibatch perplexity: 808.00\n",
      "Average loss at step 3000: 1.709662 learning rate: 0.214639\n",
      "Minibatch perplexity: 750.75\n",
      "Average loss at step 3100: 1.701900 learning rate: 0.203907\n",
      "Minibatch perplexity: 647.35\n",
      "Average loss at step 3200: 1.703835 learning rate: 0.193711\n",
      "Minibatch perplexity: 863.15\n",
      "Average loss at step 3300: 1.699227 learning rate: 0.184026\n",
      "Minibatch perplexity: 698.60\n",
      "Average loss at step 3400: 1.701825 learning rate: 0.174825\n",
      "Minibatch perplexity: 805.26\n",
      "Average loss at step 3500: 1.698564 learning rate: 0.166083\n",
      "Minibatch perplexity: 751.64\n",
      "Average loss at step 3600: 1.696551 learning rate: 0.157779\n",
      "Minibatch perplexity: 624.60\n",
      "Average loss at step 3700: 1.698295 learning rate: 0.149890\n",
      "Minibatch perplexity: 679.32\n",
      "Average loss at step 3800: 1.694241 learning rate: 0.142396\n",
      "Minibatch perplexity: 651.95\n",
      "Average loss at step 3900: 1.696377 learning rate: 0.135276\n",
      "Minibatch perplexity: 874.56\n",
      "Average loss at step 4000: 1.701049 learning rate: 0.128512\n",
      "Minibatch perplexity: 688.83\n",
      "Average loss at step 4100: 1.698281 learning rate: 0.122086\n",
      "Minibatch perplexity: 555.20\n",
      "Average loss at step 4200: 1.697521 learning rate: 0.115982\n",
      "Minibatch perplexity: 553.89\n",
      "Average loss at step 4300: 1.692064 learning rate: 0.110183\n",
      "Minibatch perplexity: 555.76\n",
      "Average loss at step 4400: 1.694461 learning rate: 0.104674\n",
      "Minibatch perplexity: 937.58\n",
      "Average loss at step 4500: 1.693192 learning rate: 0.099440\n",
      "Minibatch perplexity: 798.22\n",
      "Average loss at step 4600: 1.689056 learning rate: 0.094468\n",
      "Minibatch perplexity: 781.65\n",
      "Average loss at step 4700: 1.690402 learning rate: 0.089745\n",
      "Minibatch perplexity: 631.11\n",
      "Average loss at step 4800: 1.691897 learning rate: 0.085258\n",
      "Minibatch perplexity: 713.88\n",
      "Average loss at step 4900: 1.693932 learning rate: 0.080995\n",
      "Minibatch perplexity: 627.82\n",
      "Average loss at step 5000: 1.690863 learning rate: 0.076945\n",
      "Minibatch perplexity: 747.72\n",
      "Average loss at step 5100: 1.686416 learning rate: 0.073098\n",
      "Minibatch perplexity: 763.00\n",
      "Average loss at step 5200: 1.690506 learning rate: 0.069443\n",
      "Minibatch perplexity: 882.06\n",
      "Average loss at step 5300: 1.691773 learning rate: 0.065971\n",
      "Minibatch perplexity: 764.82\n",
      "Average loss at step 5400: 1.692502 learning rate: 0.062672\n",
      "Minibatch perplexity: 667.53\n",
      "Average loss at step 5500: 1.686062 learning rate: 0.059539\n",
      "Minibatch perplexity: 654.05\n",
      "Average loss at step 5600: 1.688804 learning rate: 0.056562\n",
      "Minibatch perplexity: 721.94\n",
      "Average loss at step 5700: 1.688642 learning rate: 0.053734\n",
      "Minibatch perplexity: 689.94\n",
      "Average loss at step 5800: 1.686411 learning rate: 0.051047\n",
      "Minibatch perplexity: 470.47\n",
      "Average loss at step 5900: 1.689248 learning rate: 0.048494\n",
      "Minibatch perplexity: 709.70\n",
      "Average loss at step 6000: 1.681822 learning rate: 0.046070\n",
      "Minibatch perplexity: 555.83\n",
      "Average loss at step 6100: 1.685557 learning rate: 0.043766\n",
      "Minibatch perplexity: 775.28\n",
      "Average loss at step 6200: 1.677766 learning rate: 0.041578\n",
      "Minibatch perplexity: 481.92\n",
      "Average loss at step 6300: 1.690320 learning rate: 0.039499\n",
      "Minibatch perplexity: 854.39\n",
      "Average loss at step 6400: 1.682902 learning rate: 0.037524\n",
      "Minibatch perplexity: 755.89\n",
      "Average loss at step 6500: 1.688770 learning rate: 0.035648\n",
      "Minibatch perplexity: 771.91\n",
      "Average loss at step 6600: 1.681822 learning rate: 0.033866\n",
      "Minibatch perplexity: 589.87\n",
      "Average loss at step 6700: 1.689854 learning rate: 0.032172\n",
      "Minibatch perplexity: 725.78\n",
      "Average loss at step 6800: 1.685991 learning rate: 0.030564\n",
      "Minibatch perplexity: 720.85\n",
      "Average loss at step 6900: 1.690335 learning rate: 0.029035\n",
      "Minibatch perplexity: 784.17\n",
      "Average loss at step 7000: 1.679677 learning rate: 0.027584\n",
      "Minibatch perplexity: 597.29\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    mean_loss = 0\n",
    "    for step in range(7001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]\n",
    "        _,l, predictions, lr, labels = sess.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate, tf_train_labels_c], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    saver.save(sess,\"model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "使用已训练的模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "Average loss at step 0: 1.750646 learning rate: 0.076945\n",
      "Minibatch perplexity: 763.36\n",
      "Average loss at step 100: 1.728438 learning rate: 0.073098\n",
      "Minibatch perplexity: 777.93\n",
      "Average loss at step 200: 1.732432 learning rate: 0.069443\n",
      "Minibatch perplexity: 664.23\n",
      "Average loss at step 300: 1.724993 learning rate: 0.065971\n",
      "Minibatch perplexity: 836.25\n",
      "Average loss at step 400: 1.732157 learning rate: 0.062672\n",
      "Minibatch perplexity: 719.91\n",
      "Average loss at step 500: 1.731018 learning rate: 0.059539\n",
      "Minibatch perplexity: 1011.49\n",
      "Average loss at step 600: 1.728173 learning rate: 0.056562\n",
      "Minibatch perplexity: 719.51\n",
      "Average loss at step 700: 1.732158 learning rate: 0.053734\n",
      "Minibatch perplexity: 748.35\n",
      "Average loss at step 800: 1.733098 learning rate: 0.051047\n",
      "Minibatch perplexity: 892.79\n",
      "Average loss at step 900: 1.731062 learning rate: 0.048494\n",
      "Minibatch perplexity: 625.37\n",
      "Average loss at step 1000: 1.728174 learning rate: 0.046070\n",
      "Minibatch perplexity: 658.18\n",
      "Average loss at step 1100: 1.732958 learning rate: 0.043766\n",
      "Minibatch perplexity: 691.10\n",
      "Average loss at step 1200: 1.726599 learning rate: 0.041578\n",
      "Minibatch perplexity: 654.13\n",
      "Average loss at step 1300: 1.730836 learning rate: 0.039499\n",
      "Minibatch perplexity: 657.89\n",
      "Average loss at step 1400: 1.726988 learning rate: 0.037524\n",
      "Minibatch perplexity: 806.81\n",
      "Average loss at step 1500: 1.726952 learning rate: 0.035648\n",
      "Minibatch perplexity: 598.54\n",
      "Average loss at step 1600: 1.732765 learning rate: 0.033866\n",
      "Minibatch perplexity: 583.47\n",
      "Average loss at step 1700: 1.729207 learning rate: 0.032172\n",
      "Minibatch perplexity: 756.64\n",
      "Average loss at step 1800: 1.725497 learning rate: 0.030564\n",
      "Minibatch perplexity: 753.14\n",
      "Average loss at step 1900: 1.727127 learning rate: 0.029035\n",
      "Minibatch perplexity: 790.73\n",
      "Average loss at step 2000: 1.732092 learning rate: 0.027584\n",
      "Minibatch perplexity: 1007.55\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess,\"model/model.ckpt\")\n",
    "    mean_loss = 0\n",
    "#     reset_step = global_step.assign(0)\n",
    "#     reset_step.eval()\n",
    "    for step in range(2001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]\n",
    "        _,l, predictions, lr, labels = sess.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate, tf_train_labels_c], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    saver.save(sess,\"model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
