{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 建立并训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from RNNCell_diy import diyLSTMCell\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pfile = './concat_image_train_data.pickle'\n",
    "with open(pfile, \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "train_dataset = train_data[\"train_dataset\"]\n",
    "train_dataset.shape = train_dataset.shape + (1,)\n",
    "train_labels = train_data[\"train_labels\"]\n",
    "train_labels_num = train_data[\"train_labels_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cdic = {\"a\":0, \"b\":1, \"c\":2, \"d\":3, \"e\":4, \"f\":5, \"g\":6, \"h\":7, \"i\":8, \"j\":9, \" \":10}\n",
    "cdic_r = {}\n",
    "for key, value in cdic.items():\n",
    "    cdic_r[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size_h = 56\n",
    "image_size_w = 112\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "构建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 5, 1, 12) dtype=float32_ref>, <tf.Variable 'bias1:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(5, 5, 12, 32) dtype=float32_ref>, <tf.Variable 'bias3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(4608, 300) dtype=float32_ref>, <tf.Variable 'bias5:0' shape=(300,) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(300, 260) dtype=float32_ref>, <tf.Variable 'bias6:0' shape=(260,) dtype=float32_ref>, <tf.Variable 'Variable_4:0' shape=(260, 3) dtype=float32_ref>, <tf.Variable 's_bias:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'Variable_5:0' shape=(260, 11) dtype=float32_ref>, <tf.Variable 'c1_bias:0' shape=(11,) dtype=float32_ref>, <tf.Variable 'Variable_6:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c2_bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'Variable_7:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c3_bias:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "kernel_size = 5\n",
    "pooling_size = 2\n",
    "channels_one = 12\n",
    "channels_two = 32\n",
    "hidden1_size = 300\n",
    "hidden2_size = 260\n",
    "dropout = 0.5\n",
    "lam = 0.01\n",
    "max_time_step = 3\n",
    "rnn_num_nodes = 260\n",
    "rnn_num_layers = 3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size_h, image_size_w, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32,shape=(batch_size, max_time_step))\n",
    "    tf_train_labels_num = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    \n",
    "    #variables\n",
    "    layer_weight1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, num_channels, channels_one],stddev=0.1))\n",
    "    layer_biases1 = tf.Variable(tf.zeros([channels_one]), name=\"bias1\")\n",
    "    \n",
    "    layer_weight3 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels_one, channels_two], stddev=0.1))\n",
    "    layer_biases3 = tf.Variable(tf.constant(1.0, shape=[channels_two]), name=\"bias3\")\n",
    "    \n",
    "    neuron_num = ((((image_size_h-pooling_size)//pooling_size+1-kernel_size+1-pooling_size)//pooling_size+1)**2)*channels_two\n",
    "    layer_weight5 = tf.Variable(tf.truncated_normal([neuron_num, hidden1_size], stddev=0.1))\n",
    "    layer_biases5 = tf.Variable(tf.constant(1.0, shape=[hidden1_size]), name=\"bias5\")\n",
    "    \n",
    "    layer_weight6 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "    layer_biases6 = tf.Variable(tf.constant(1.0, shape=[hidden2_size]), name=\"bias6\")\n",
    "    \n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    s_weight = tf.Variable(tf.truncated_normal([hidden2_size, max_time_step], stddev=0.1))\n",
    "    s_biases = tf.Variable(tf.constant(1.0, shape=[max_time_step]), name=\"s_bias\")\n",
    "    \n",
    "    c1_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)], stddev=0.1))\n",
    "    c1_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)]), name=\"c1_bias\")\n",
    "    \n",
    "    c2_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1))\n",
    "    c2_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c2_bias\")\n",
    "    \n",
    "    c3_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1))\n",
    "    c3_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c3_bias\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     w = tf.Variable(tf.truncated_normal([rnn_num_nodes, len(cdic)], -0.1, 0.1))\n",
    "#     b = tf.Variable(tf.zeros([len(cdic)]), name=\"bias_o\")\n",
    "    \n",
    "    # Model\n",
    "    def conv_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer_weight1, [1,1,1,1], padding=\"SAME\")+layer_biases1\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,4,1], [1,2,4,1], padding=\"VALID\"))\n",
    "        conv = tf.nn.conv2d(pooling, layer_weight3, [1,1,1,1], padding=\"VALID\")+layer_biases3\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding=\"VALID\"))\n",
    "        shape = pooling.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling,[shape[0],shape[1]*shape[2]*shape[3]])\n",
    "        hidden1 = tf.nn.dropout(tf.matmul(reshape, layer_weight5)+layer_biases5, dropout)\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, layer_weight6)+layer_biases6)\n",
    "        return hidden2\n",
    "            \n",
    "#     def rnn_model1(input_data):\n",
    "#         cell_list = []\n",
    "#         for i in range(rnn_num_layers):\n",
    "#             rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "#             rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "#             cell_list.append(rnn_cell)\n",
    "#         rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "#         with tf.variable_scope(\"rnn1\"):\n",
    "#             rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "#                 rnn_cells,\n",
    "#                 input_data,\n",
    "#                 dtype=tf.float32)\n",
    "#         return rnn_outputs, rnn_state\n",
    "    \n",
    "#     def rnn_model2(input_data, init_state):\n",
    "#         cell_list = []\n",
    "#         for i in range(rnn_num_layers):\n",
    "#             rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "#             rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "#             cell_list.append(rnn_cell)\n",
    "#         rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "#         with tf.variable_scope(\"rnn2\"):\n",
    "#             rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "#                 rnn_cells,\n",
    "#                 input_data,\n",
    "#                 dtype=tf.float32,\n",
    "#                 initial_state=init_state,\n",
    "#                 sequence_length=tf_train_labels_num+1,\n",
    "#                 swap_memory=True)\n",
    "#         return rnn_outputs, rnn_state\n",
    "        \n",
    "    \n",
    "    #loss\n",
    "    conv_out = conv_model(tf_train_dataset)\n",
    "#     conv_out = tf.reshape(conv_out,[batch_size,1,-1])\n",
    "#     rnn1_output, rnn1_state = rnn_model1(conv_out)\n",
    "#     input_data = tf.concat((tf.constant(cdic[\"remain\"],shape=[batch_size,1]), tf_train_labels),1)\n",
    "#     input_data = tf.one_hot(input_data,len(cdic))\n",
    "#     tf_train_labels_c = tf.concat((tf_train_labels,tf.constant(cdic[\"remain\"],shape=[batch_size,1])),1)\n",
    "#     tf_train_labels_c = tf.one_hot(tf_train_labels_c,len(cdic))\n",
    "#     rnn2_outputs, _ = rnn_model2(input_data, rnn1_state)\n",
    "#     rnn2_outputs = tf.reshape(rnn2_outputs,[-1, rnn_num_nodes])\n",
    "#     logits = tf.reshape(tf.matmul(rnn2_outputs, w) + b, [batch_size, max_time_step+1,-1])\n",
    "\n",
    "    tf_train_labels_c1 = tf_train_labels[:,0]\n",
    "    tf_train_labels_c1 = tf.one_hot(tf_train_labels_c1, len(cdic))\n",
    "    c1_logits = tf.matmul(conv_out, c1_weight)+c1_biases\n",
    "    \n",
    "    tf_train_labels_c2 = tf_train_labels[:,1]\n",
    "    mask = tf.logical_not(tf.equal(tf_train_labels_c2, cdic[\" \"])) # boolean tensor, mask[i] = True iff x[i] ==cdic[\" \"]\n",
    "    tf_train_labels_c2 = tf.boolean_mask(tf_train_labels_c2, mask)\n",
    "    tf_train_labels_c2 = tf.one_hot(tf_train_labels_c2, len(cdic)-1)\n",
    "#     c2_logits = tf.matmul(conv_out, c2_weight)+c2_biases\n",
    "    c2_logits = tf.matmul(tf.boolean_mask(conv_out, mask), c2_weight)+c2_biases\n",
    "\n",
    "    \n",
    "    tf_train_labels_c3 = tf_train_labels[:,2]\n",
    "    mask = tf.logical_not(tf.equal(tf_train_labels_c3, cdic[\" \"])) # boolean tensor, mask[i] = True iff x[i] ==cdic[\" \"]\n",
    "    tf_train_labels_c3 = tf.boolean_mask(tf_train_labels_c3, mask)\n",
    "    tf_train_labels_c3 = tf.one_hot(tf_train_labels_c3, len(cdic)-1)\n",
    "#     c3_logits = tf.matmul(conv_out, c3_weight)+c3_biases\n",
    "    c3_logits = tf.matmul(tf.boolean_mask(conv_out, mask), c3_weight)+c3_biases\n",
    "    \n",
    "    tf_train_labels_s = tf.one_hot(tf_train_labels_num, max_time_step)\n",
    "    s_logits = tf.matmul(conv_out, s_weight)+s_biases\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if not(\"bias\" in v.name)])\n",
    "    \n",
    "    mu3 = tf.cast(tf.shape(tf_train_labels_c3)[0], tf.float32)/batch_size\n",
    "\n",
    "\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c1, logits=c1_logits)) \\\n",
    "        +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c2, logits=c2_logits)) \\\n",
    "        +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c3, logits=c3_logits)) \\\n",
    "        +0.1*tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_s, logits=s_logits)) \\\n",
    "        +lam*regularization_cost\n",
    "        \n",
    "#     loss = mu3*(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c3, logits=c3_logits))+lam*regularization_cost)\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c2, logits=c2_logits)) \\\n",
    "#         +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_s, logits=s_logits)) \\\n",
    "#         +lam*regularization_cost\n",
    "    \n",
    "    # Optimizer.\n",
    "#     global_step = tf.Variable(0)\n",
    "#     learning_rate = tf.train.exponential_decay(\n",
    "#       10.0, global_step, 100, 0.95, staircase=True)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "#     gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "#     optimizer = optimizer.apply_gradients(\n",
    "#       zip(gradients, v), global_step=global_step)\n",
    "    learning_rate = tf.constant(1e-7)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction1 = tf.nn.softmax(c1_logits)\n",
    "    train_prediction2 = tf.nn.softmax(c2_logits)\n",
    "    train_prediction3 = tf.nn.softmax(c3_logits)\n",
    "    train_prediction_s = tf.nn.softmax(s_logits)\n",
    "    \n",
    "    #model save\n",
    "    tv = tf.trainable_variables()\n",
    "    print(tv)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "第一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 125.949852 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 54.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 2.00%\n",
      "Minibatch accuracy3: 8.00%\n",
      "Average loss at step 100: 65.065210 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 46.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 14.00%\n",
      "Minibatch accuracy3: 9.52%\n",
      "Average loss at step 200: 20.643609 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 52.00%\n",
      "Minibatch accuracy1: 4.00%\n",
      "Minibatch accuracy2: 10.00%\n",
      "Minibatch accuracy3: 3.85%\n",
      "Average loss at step 300: 12.250149 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 50.00%\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 18.00%\n",
      "Minibatch accuracy3: 9.52%\n",
      "Average loss at step 400: 10.270074 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 58.00%\n",
      "Minibatch accuracy1: 16.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 7.41%\n",
      "Average loss at step 500: 9.508391 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 54.00%\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 15.38%\n",
      "Average loss at step 600: 9.045514 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 66.00%\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 4.76%\n",
      "Average loss at step 700: 8.673652 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 48.00%\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 16.67%\n",
      "Average loss at step 800: 8.379307 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 52.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 11.11%\n",
      "Average loss at step 900: 8.140206 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 56.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 14.00%\n",
      "Minibatch accuracy3: 0.00%\n",
      "Average loss at step 1000: 7.941422 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 60.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 6.67%\n",
      "Average loss at step 1100: 7.779036 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 38.00%\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 10.53%\n",
      "Average loss at step 1200: 7.643755 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 60.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 0.00%\n",
      "Minibatch accuracy3: 5.00%\n",
      "Average loss at step 1300: 7.538957 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 58.00%\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 2.00%\n",
      "Minibatch accuracy3: 9.09%\n",
      "Average loss at step 1400: 7.451939 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 50.00%\n",
      "Minibatch accuracy1: 20.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 4.00%\n",
      "Average loss at step 1500: 7.372794 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 58.00%\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 10.00%\n",
      "Minibatch accuracy3: 6.90%\n",
      "Average loss at step 1600: 7.310394 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 46.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 8.70%\n",
      "Average loss at step 1700: 7.260750 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 54.00%\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 13.04%\n",
      "Average loss at step 1800: 7.224786 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 46.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 4.17%\n",
      "Average loss at step 1900: 7.234490 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 62.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 0.00%\n",
      "Average loss at step 2000: 7.234847 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 52.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 7.69%\n",
      "Average loss at step 2100: 7.151558 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 50.00%\n",
      "Minibatch accuracy1: 16.00%\n",
      "Minibatch accuracy2: 22.00%\n",
      "Minibatch accuracy3: 8.00%\n",
      "Average loss at step 2200: 7.185779 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 48.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 15.62%\n",
      "Average loss at step 2300: 7.204948 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 50.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 20.00%\n",
      "Minibatch accuracy3: 8.33%\n",
      "Average loss at step 2400: 7.178635 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 62.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 9.68%\n",
      "Average loss at step 2500: 7.243418 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 46.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 4.00%\n",
      "Average loss at step 2600: 7.122136 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 54.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 11.11%\n",
      "Average loss at step 2700: 7.240636 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 36.00%\n",
      "Minibatch accuracy1: 2.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 5.56%\n",
      "Average loss at step 2800: 7.112608 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 50.00%\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 10.00%\n",
      "Minibatch accuracy3: 0.00%\n",
      "Average loss at step 2900: 7.160038 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 54.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 7.69%\n",
      "Average loss at step 3000: 7.191142 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 48.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 12.50%\n",
      "Average loss at step 3100: 7.176897 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 58.00%\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 14.00%\n",
      "Minibatch accuracy3: 17.24%\n",
      "Average loss at step 3200: 7.637876 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 64.00%\n",
      "Minibatch accuracy1: 20.00%\n",
      "Minibatch accuracy2: 18.00%\n",
      "Minibatch accuracy3: 40.00%\n",
      "Average loss at step 3300: 8.148330 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 58.00%\n",
      "Minibatch accuracy1: 42.00%\n",
      "Minibatch accuracy2: 36.00%\n",
      "Minibatch accuracy3: 29.17%\n",
      "Average loss at step 3400: 7.330369 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 74.00%\n",
      "Minibatch accuracy1: 44.00%\n",
      "Minibatch accuracy2: 26.00%\n",
      "Minibatch accuracy3: 28.57%\n",
      "Average loss at step 3500: 6.936521 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 84.00%\n",
      "Minibatch accuracy1: 22.00%\n",
      "Minibatch accuracy2: 24.00%\n",
      "Minibatch accuracy3: 28.57%\n",
      "Average loss at step 3600: 6.625535 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 64.00%\n",
      "Minibatch accuracy1: 46.00%\n",
      "Minibatch accuracy2: 36.00%\n",
      "Minibatch accuracy3: 40.91%\n",
      "Average loss at step 3700: 6.336400 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 48.00%\n",
      "Minibatch accuracy2: 40.00%\n",
      "Minibatch accuracy3: 70.00%\n",
      "Average loss at step 3800: 6.130456 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 88.00%\n",
      "Minibatch accuracy1: 44.00%\n",
      "Minibatch accuracy2: 40.00%\n",
      "Minibatch accuracy3: 52.00%\n",
      "Average loss at step 3900: 5.978735 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 84.00%\n",
      "Minibatch accuracy1: 70.00%\n",
      "Minibatch accuracy2: 54.00%\n",
      "Minibatch accuracy3: 42.31%\n",
      "Average loss at step 4000: 5.922705 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 38.00%\n",
      "Minibatch accuracy2: 40.00%\n",
      "Minibatch accuracy3: 36.67%\n",
      "Average loss at step 4100: 5.781841 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 52.00%\n",
      "Minibatch accuracy2: 42.00%\n",
      "Minibatch accuracy3: 45.16%\n",
      "Average loss at step 4200: 5.866176 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 84.00%\n",
      "Minibatch accuracy1: 46.00%\n",
      "Minibatch accuracy2: 34.00%\n",
      "Minibatch accuracy3: 42.31%\n",
      "Average loss at step 4300: 5.564940 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 62.00%\n",
      "Minibatch accuracy2: 40.00%\n",
      "Minibatch accuracy3: 60.00%\n",
      "Average loss at step 4400: 5.639492 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 88.00%\n",
      "Minibatch accuracy1: 34.00%\n",
      "Minibatch accuracy2: 58.00%\n",
      "Minibatch accuracy3: 50.00%\n",
      "Average loss at step 4500: 5.506312 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 88.00%\n",
      "Minibatch accuracy1: 58.00%\n",
      "Minibatch accuracy2: 44.00%\n",
      "Minibatch accuracy3: 56.00%\n",
      "Average loss at step 4600: 5.365420 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 56.00%\n",
      "Minibatch accuracy3: 65.22%\n",
      "Average loss at step 4700: 5.568994 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 96.00%\n",
      "Minibatch accuracy1: 44.00%\n",
      "Minibatch accuracy2: 44.00%\n",
      "Minibatch accuracy3: 50.00%\n",
      "Average loss at step 4800: 5.441523 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 44.00%\n",
      "Minibatch accuracy2: 50.00%\n",
      "Minibatch accuracy3: 61.54%\n",
      "Average loss at step 4900: 5.596049 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 68.00%\n",
      "Minibatch accuracy2: 46.00%\n",
      "Minibatch accuracy3: 68.18%\n",
      "Average loss at step 5000: 5.406606 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 92.00%\n",
      "Minibatch accuracy1: 54.00%\n",
      "Minibatch accuracy2: 46.00%\n",
      "Minibatch accuracy3: 60.71%\n",
      "Average loss at step 5100: 5.676467 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 52.00%\n",
      "Minibatch accuracy2: 52.00%\n",
      "Minibatch accuracy3: 58.33%\n",
      "Average loss at step 5200: 5.494391 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 68.00%\n",
      "Minibatch accuracy2: 60.00%\n",
      "Minibatch accuracy3: 66.67%\n",
      "Average loss at step 5300: 5.590883 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 86.00%\n",
      "Minibatch accuracy1: 58.00%\n",
      "Minibatch accuracy2: 68.00%\n",
      "Minibatch accuracy3: 47.83%\n",
      "Average loss at step 5400: 6.986332 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 62.00%\n",
      "Minibatch accuracy2: 50.00%\n",
      "Minibatch accuracy3: 54.17%\n",
      "Average loss at step 5500: 5.747742 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 86.00%\n",
      "Minibatch accuracy1: 54.00%\n",
      "Minibatch accuracy2: 60.00%\n",
      "Minibatch accuracy3: 56.00%\n",
      "Average loss at step 5600: 5.669177 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 96.00%\n",
      "Minibatch accuracy1: 42.00%\n",
      "Minibatch accuracy2: 46.00%\n",
      "Minibatch accuracy3: 62.96%\n",
      "Average loss at step 5700: 5.626822 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 42.00%\n",
      "Minibatch accuracy3: 45.83%\n",
      "Average loss at step 5800: 5.537294 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 92.00%\n",
      "Minibatch accuracy1: 64.00%\n",
      "Minibatch accuracy2: 52.00%\n",
      "Minibatch accuracy3: 59.26%\n",
      "Average loss at step 5900: 5.459066 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 92.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 54.00%\n",
      "Minibatch accuracy3: 45.45%\n",
      "Average loss at step 6000: 5.535048 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 92.00%\n",
      "Minibatch accuracy1: 50.00%\n",
      "Minibatch accuracy2: 58.00%\n",
      "Minibatch accuracy3: 50.00%\n",
      "Average loss at step 6100: 5.432332 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 56.00%\n",
      "Minibatch accuracy3: 56.00%\n",
      "Average loss at step 6200: 5.496421 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 46.00%\n",
      "Minibatch accuracy3: 46.88%\n",
      "Average loss at step 6300: 5.757605 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 96.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 40.00%\n",
      "Minibatch accuracy3: 61.90%\n",
      "Average loss at step 6400: 5.382234 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 94.00%\n",
      "Minibatch accuracy1: 68.00%\n",
      "Minibatch accuracy2: 64.00%\n",
      "Minibatch accuracy3: 57.89%\n",
      "Average loss at step 6500: 5.822226 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 62.00%\n",
      "Minibatch accuracy2: 62.00%\n",
      "Minibatch accuracy3: 38.46%\n",
      "Average loss at step 6600: 5.728066 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 70.00%\n",
      "Minibatch accuracy2: 54.00%\n",
      "Minibatch accuracy3: 65.22%\n",
      "Average loss at step 6700: 5.553283 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 96.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 62.00%\n",
      "Minibatch accuracy3: 60.87%\n",
      "Average loss at step 6800: 5.349369 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 92.00%\n",
      "Minibatch accuracy1: 74.00%\n",
      "Minibatch accuracy2: 56.00%\n",
      "Minibatch accuracy3: 65.38%\n",
      "Average loss at step 6900: 5.296620 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 90.00%\n",
      "Minibatch accuracy1: 56.00%\n",
      "Minibatch accuracy2: 66.00%\n",
      "Minibatch accuracy3: 69.57%\n",
      "Average loss at step 7000: 5.040614 learning rate: 0.0099999998\n",
      "Minibatch accuracy_s: 86.00%\n",
      "Minibatch accuracy1: 64.00%\n",
      "Minibatch accuracy2: 52.00%\n",
      "Minibatch accuracy3: 64.00%\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "with graph.as_default():\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    mean_loss = 0\n",
    "    step = 0\n",
    "    l = 100\n",
    "    for step in range(7001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]-1\n",
    "        _,l, predictions1, predictions2, predictions3, predictions_s, lr, labels1, labels2, labels3, labels_s = sess.run(\n",
    "          [optimizer, loss, train_prediction1, train_prediction2, train_prediction3, train_prediction_s,\n",
    "           learning_rate,\n",
    "           tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3, tf_train_labels_s], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %.10f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch accuracy_s: %.2f%%' % accuracy(predictions_s, labels_s))\n",
    "            print('Minibatch accuracy1: %.2f%%' % accuracy(predictions1, labels1))\n",
    "            print('Minibatch accuracy2: %.2f%%' % accuracy(predictions2, labels2))\n",
    "            print('Minibatch accuracy3: %.2f%%' % accuracy(predictions3, labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.save(sess,\"model3/model.ckpt\")\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "使用已训练的模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model3/model.ckpt\n",
      "Average loss at step 0: 2.522596 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 96.00%\n",
      "Minibatch accuracy1: 84.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 87.10%\n",
      "Average loss at step 100: 2.305898 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 88.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 80.00%\n",
      "Average loss at step 200: 2.351716 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 90.00%\n",
      "Minibatch accuracy2: 94.00%\n",
      "Minibatch accuracy3: 92.00%\n",
      "Average loss at step 300: 2.311377 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 88.00%\n",
      "Minibatch accuracy2: 84.00%\n",
      "Minibatch accuracy3: 92.00%\n",
      "Average loss at step 400: 2.308612 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 80.00%\n",
      "Minibatch accuracy3: 91.67%\n",
      "Average loss at step 500: 2.291946 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 80.00%\n",
      "Minibatch accuracy3: 73.68%\n",
      "Average loss at step 600: 2.217388 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 80.00%\n",
      "Minibatch accuracy2: 74.00%\n",
      "Minibatch accuracy3: 84.00%\n",
      "Average loss at step 700: 2.346635 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 86.00%\n",
      "Minibatch accuracy2: 88.00%\n",
      "Minibatch accuracy3: 84.62%\n",
      "Average loss at step 800: 2.345799 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 80.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 96.00%\n",
      "Average loss at step 900: 2.267376 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 74.00%\n",
      "Minibatch accuracy2: 88.00%\n",
      "Minibatch accuracy3: 82.61%\n",
      "Average loss at step 1000: 2.300662 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 86.00%\n",
      "Minibatch accuracy2: 84.00%\n",
      "Minibatch accuracy3: 93.10%\n",
      "Average loss at step 1100: 2.319477 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 72.00%\n",
      "Minibatch accuracy2: 84.00%\n",
      "Minibatch accuracy3: 78.26%\n",
      "Average loss at step 1200: 2.294107 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 84.00%\n",
      "Minibatch accuracy2: 92.00%\n",
      "Minibatch accuracy3: 76.92%\n",
      "Average loss at step 1300: 2.264791 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 88.00%\n",
      "Minibatch accuracy2: 70.00%\n",
      "Minibatch accuracy3: 85.19%\n",
      "Average loss at step 1400: 2.366406 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 90.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 95.65%\n",
      "Average loss at step 1500: 2.344574 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 88.00%\n",
      "Minibatch accuracy2: 76.00%\n",
      "Minibatch accuracy3: 84.62%\n",
      "Average loss at step 1600: 2.306888 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 78.00%\n",
      "Minibatch accuracy3: 100.00%\n",
      "Average loss at step 1700: 2.299081 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 80.00%\n",
      "Minibatch accuracy3: 91.67%\n",
      "Average loss at step 1800: 2.293302 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 78.00%\n",
      "Minibatch accuracy3: 92.31%\n",
      "Average loss at step 1900: 2.309268 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 90.00%\n",
      "Minibatch accuracy2: 84.00%\n",
      "Minibatch accuracy3: 90.32%\n",
      "Average loss at step 2000: 2.280021 learning rate: 0.0000001000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 88.00%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess,\"model3/model.ckpt\")\n",
    "    mean_loss = 0\n",
    "#     reset_step = global_step.assign(500)\n",
    "#     reset_step.eval()\n",
    "    for step in range(2001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]-1\n",
    "        _,l, predictions1, predictions2, predictions3, predictions_s, lr, labels1, labels2, labels3, labels_s = sess.run(\n",
    "          [optimizer, loss, train_prediction1, train_prediction2, train_prediction3, train_prediction_s,\n",
    "           learning_rate,\n",
    "           tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3, tf_train_labels_s], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %.10f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch accuracy_s: %.2f%%' % accuracy(predictions_s, labels_s))\n",
    "            print('Minibatch accuracy1: %.2f%%' % accuracy(predictions1, labels1))\n",
    "            print('Minibatch accuracy2: %.2f%%' % accuracy(predictions2, labels2))\n",
    "            print('Minibatch accuracy3: %.2f%%' % accuracy(predictions3, labels3))\n",
    "    saver.save(sess,\"model3/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
