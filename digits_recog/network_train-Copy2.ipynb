{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 建立并训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from RNNCell_diy import diyLSTMCell\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pfile = './concat_image_train_data.pickle'\n",
    "with open(pfile, \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "train_dataset = train_data[\"train_dataset\"]\n",
    "train_dataset.shape = train_dataset.shape + (1,)\n",
    "train_labels = train_data[\"train_labels\"]\n",
    "train_labels_num = train_data[\"train_labels_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cdic = {\"a\":0, \"b\":1, \"c\":2, \"d\":3, \"e\":4, \"f\":5, \"g\":6, \"h\":7, \"i\":8, \"j\":9, \" \":10}\n",
    "cdic_r = {}\n",
    "for key, value in cdic.items():\n",
    "    cdic_r[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size_h = 56\n",
    "image_size_w = 112\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "构建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 5, 1, 12) dtype=float32_ref>, <tf.Variable 'bias1:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(5, 5, 12, 32) dtype=float32_ref>, <tf.Variable 'bias3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(4608, 300) dtype=float32_ref>, <tf.Variable 'bias5:0' shape=(300,) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(300, 260) dtype=float32_ref>, <tf.Variable 'bias6:0' shape=(260,) dtype=float32_ref>, <tf.Variable 'c1_weight:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c1_bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'c2_weight:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c2_bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'c3_weight:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c3_bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'Variable_4:0' shape=() dtype=int32_ref>]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "kernel_size = 5\n",
    "pooling_size = 2\n",
    "channels_one = 12\n",
    "channels_two = 32\n",
    "hidden1_size = 300\n",
    "hidden2_size = 260\n",
    "dropout = 0.5\n",
    "lam = 0.01\n",
    "max_time_step = 3\n",
    "rnn_num_nodes = 260\n",
    "rnn_num_layers = 3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size_h, image_size_w, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32,shape=(batch_size, max_time_step))\n",
    "    tf_train_labels_num = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    \n",
    "    #variables\n",
    "    layer_weight1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, num_channels, channels_one],stddev=0.1))\n",
    "    layer_biases1 = tf.Variable(tf.zeros([channels_one]), name=\"bias1\")\n",
    "    \n",
    "    layer_weight3 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels_one, channels_two], stddev=0.1))\n",
    "    layer_biases3 = tf.Variable(tf.constant(1.0, shape=[channels_two]), name=\"bias3\")\n",
    "    \n",
    "    neuron_num = ((((image_size_h-pooling_size)//pooling_size+1-kernel_size+1-pooling_size)//pooling_size+1)**2)*channels_two\n",
    "    layer_weight5 = tf.Variable(tf.truncated_normal([neuron_num, hidden1_size], stddev=0.1))\n",
    "    layer_biases5 = tf.Variable(tf.constant(1.0, shape=[hidden1_size]), name=\"bias5\")\n",
    "    \n",
    "    layer_weight6 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "    layer_biases6 = tf.Variable(tf.constant(1.0, shape=[hidden2_size]), name=\"bias6\")\n",
    "    \n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "#     s_weight = tf.Variable(tf.truncated_normal([hidden2_size, max_time_step], stddev=0.1), name=\"s_weight\")\n",
    "#     s_biases = tf.Variable(tf.constant(1.0, shape=[max_time_step]), name=\"s_bias\")\n",
    "    \n",
    "    c1_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1), name=\"c1_weight\")\n",
    "    c1_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c1_bias\")\n",
    "    \n",
    "    c2_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1), name=\"c2_weight\")\n",
    "    c2_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c2_bias\")\n",
    "    \n",
    "    c3_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1), name=\"c3_weight\")\n",
    "    c3_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c3_bias\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     w = tf.Variable(tf.truncated_normal([rnn_num_nodes, len(cdic)], -0.1, 0.1))\n",
    "#     b = tf.Variable(tf.zeros([len(cdic)]), name=\"bias_o\")\n",
    "    \n",
    "    # Model\n",
    "    def conv_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer_weight1, [1,1,1,1], padding=\"SAME\")+layer_biases1\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,4,1], [1,2,4,1], padding=\"VALID\"))\n",
    "        conv = tf.nn.conv2d(pooling, layer_weight3, [1,1,1,1], padding=\"VALID\")+layer_biases3\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding=\"VALID\"))\n",
    "        shape = pooling.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling,[shape[0],shape[1]*shape[2]*shape[3]])\n",
    "        hidden1 = tf.nn.dropout(tf.matmul(reshape, layer_weight5)+layer_biases5, dropout)\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, layer_weight6)+layer_biases6)\n",
    "        return hidden2\n",
    "            \n",
    "#     def rnn_model1(input_data):\n",
    "#         cell_list = []\n",
    "#         for i in range(rnn_num_layers):\n",
    "#             rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "#             rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "#             cell_list.append(rnn_cell)\n",
    "#         rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "#         with tf.variable_scope(\"rnn1\"):\n",
    "#             rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "#                 rnn_cells,\n",
    "#                 input_data,\n",
    "#                 dtype=tf.float32)\n",
    "#         return rnn_outputs, rnn_state\n",
    "    \n",
    "#     def rnn_model2(input_data, init_state):\n",
    "#         cell_list = []\n",
    "#         for i in range(rnn_num_layers):\n",
    "#             rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "#             rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "#             cell_list.append(rnn_cell)\n",
    "#         rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "#         with tf.variable_scope(\"rnn2\"):\n",
    "#             rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "#                 rnn_cells,\n",
    "#                 input_data,\n",
    "#                 dtype=tf.float32,\n",
    "#                 initial_state=init_state,\n",
    "#                 sequence_length=tf_train_labels_num+1,\n",
    "#                 swap_memory=True)\n",
    "#         return rnn_outputs, rnn_state\n",
    "        \n",
    "    \n",
    "    #loss\n",
    "    conv_out = conv_model(tf_train_dataset)\n",
    "#     conv_out = tf.reshape(conv_out,[batch_size,1,-1])\n",
    "#     rnn1_output, rnn1_state = rnn_model1(conv_out)\n",
    "#     input_data = tf.concat((tf.constant(cdic[\"remain\"],shape=[batch_size,1]), tf_train_labels),1)\n",
    "#     input_data = tf.one_hot(input_data,len(cdic))\n",
    "#     tf_train_labels_c = tf.concat((tf_train_labels,tf.constant(cdic[\"remain\"],shape=[batch_size,1])),1)\n",
    "#     tf_train_labels_c = tf.one_hot(tf_train_labels_c,len(cdic))\n",
    "#     rnn2_outputs, _ = rnn_model2(input_data, rnn1_state)\n",
    "#     rnn2_outputs = tf.reshape(rnn2_outputs,[-1, rnn_num_nodes])\n",
    "#     logits = tf.reshape(tf.matmul(rnn2_outputs, w) + b, [batch_size, max_time_step+1,-1])\n",
    "\n",
    "    tf_train_labels_c1 = tf_train_labels[:,0]\n",
    "    tf_train_labels_c1 = tf.one_hot(tf_train_labels_c1, len(cdic)-1)\n",
    "    c1_logits = tf.matmul(conv_out, c1_weight)+c1_biases\n",
    "    \n",
    "    tf_train_labels_c2 = tf_train_labels[:,1]\n",
    "    mask = tf.logical_not(tf.equal(tf_train_labels_c2, cdic[\" \"])) # boolean tensor, mask[i] = True iff x[i] ==cdic[\" \"]\n",
    "    tf_train_labels_c2 = tf.boolean_mask(tf_train_labels_c2, mask)\n",
    "    tf_train_labels_c2 = tf.one_hot(tf_train_labels_c2, len(cdic)-1)\n",
    "#     c2_logits = tf.matmul(conv_out, c2_weight)+c2_biases\n",
    "    c2_logits = tf.matmul(tf.boolean_mask(conv_out, mask), c2_weight)+c2_biases\n",
    "\n",
    "    \n",
    "    tf_train_labels_c3 = tf_train_labels[:,2]\n",
    "    mask = tf.logical_not(tf.equal(tf_train_labels_c3, cdic[\" \"])) # boolean tensor, mask[i] = True iff x[i] ==cdic[\" \"]\n",
    "    tf_train_labels_c3 = tf.boolean_mask(tf_train_labels_c3, mask)\n",
    "    tf_train_labels_c3 = tf.one_hot(tf_train_labels_c3, len(cdic)-1)\n",
    "#     c3_logits = tf.matmul(conv_out, c3_weight)+c3_biases\n",
    "    c3_logits = tf.matmul(tf.boolean_mask(conv_out, mask), c3_weight)+c3_biases\n",
    "    \n",
    "#     tf_train_labels_s = tf.one_hot(tf_train_labels_num, max_time_step)\n",
    "#     s_logits = tf.matmul(conv_out, s_weight)+s_biases\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    common_regu_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if not(\"bias\" in v.name)\n",
    "                                      and not(\"s_\" in v.name or \"c1_\" in v.name or \"c2_\" in v.name or \"c3_\" in v.name)])\n",
    "    regularization_cost1 = common_regu_cost + tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if (\"c1_\" in v.name)])\n",
    "    regularization_cost2 = common_regu_cost + tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if (\"c2_\" in v.name)])\n",
    "    regularization_cost3 = common_regu_cost + tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if (\"c3_\" in v.name)])\n",
    "    \n",
    "    loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c1, logits=c1_logits)) + regularization_cost1\n",
    "    loss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c2, logits=c2_logits)) + regularization_cost2\n",
    "    loss3 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c3, logits=c3_logits)) + regularization_cost3\n",
    "#         +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_s, logits=s_logits)) \\ \n",
    "\n",
    "    global_loss = loss1+loss2+loss3\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "      1.0, global_step, 100, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients1, v1 = zip(*optimizer.compute_gradients(loss1))\n",
    "    gradients1, _ = tf.clip_by_global_norm(gradients1, 1.25)\n",
    "    optimizer1 = optimizer.apply_gradients(\n",
    "      zip(gradients1, v1), global_step=global_step)\n",
    "    \n",
    "\n",
    "\n",
    "    gradients2, v2 = zip(*optimizer.compute_gradients(loss2))\n",
    "    gradients2, _ = tf.clip_by_global_norm(gradients2, 1.25)\n",
    "    optimizer2 = optimizer.apply_gradients(\n",
    "      zip(gradients2, v2))\n",
    "    \n",
    "\n",
    "\n",
    "    gradients3, v3 = zip(*optimizer.compute_gradients(loss3))\n",
    "    gradients3, _ = tf.clip_by_global_norm(gradients3, 1.25)\n",
    "    optimizer3 = optimizer.apply_gradients(\n",
    "      zip(gradients3, v3))\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction1 = tf.nn.softmax(c1_logits)\n",
    "    train_prediction2 = tf.nn.softmax(c2_logits)\n",
    "    train_prediction3 = tf.nn.softmax(c3_logits)\n",
    "#     train_prediction_s = tf.nn.softmax(s_logits)\n",
    "    \n",
    "    #model save\n",
    "    print(tv)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "第一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 17138.742188 learning rate: 1.000000\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 18.00%\n",
      "Minibatch accuracy3: 0.00%\n",
      "Average loss at step 100: 1751.083562 learning rate: 0.950000\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 3.23%\n",
      "Average loss at step 200: 11.637202 learning rate: 0.902500\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 0.00%\n",
      "Average loss at step 300: 11.164996 learning rate: 0.857375\n",
      "Minibatch accuracy1: 2.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 10.00%\n",
      "Average loss at step 400: 10.745086 learning rate: 0.814506\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 24.00%\n",
      "Minibatch accuracy3: 7.41%\n",
      "Average loss at step 500: 10.368782 learning rate: 0.773781\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 4.00%\n",
      "Average loss at step 600: 10.024351 learning rate: 0.735092\n",
      "Minibatch accuracy1: 20.00%\n",
      "Minibatch accuracy2: 10.00%\n",
      "Minibatch accuracy3: 12.00%\n",
      "Average loss at step 700: 9.721167 learning rate: 0.698337\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 10.00%\n",
      "Minibatch accuracy3: 4.55%\n",
      "Average loss at step 800: 9.440493 learning rate: 0.663420\n",
      "Minibatch accuracy1: 4.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 13.04%\n",
      "Average loss at step 900: 9.296203 learning rate: 0.630249\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 2.00%\n",
      "Minibatch accuracy3: 9.09%\n",
      "Average loss at step 1000: 10.017882 learning rate: 0.598737\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 14.29%\n",
      "Average loss at step 1100: 10.036387 learning rate: 0.568800\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 16.67%\n",
      "Average loss at step 1200: 9.219904 learning rate: 0.540360\n",
      "Minibatch accuracy1: 6.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 14.81%\n",
      "Average loss at step 1300: 8.718187 learning rate: 0.513342\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 4.76%\n",
      "Average loss at step 1400: 8.737034 learning rate: 0.487675\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 3.85%\n",
      "Average loss at step 1500: 8.400401 learning rate: 0.463291\n",
      "Minibatch accuracy1: 24.00%\n",
      "Minibatch accuracy2: 10.00%\n",
      "Minibatch accuracy3: 10.53%\n",
      "Average loss at step 1600: 8.188733 learning rate: 0.440127\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 23.81%\n",
      "Average loss at step 1700: 7.826743 learning rate: 0.418120\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 14.00%\n",
      "Minibatch accuracy3: 13.64%\n",
      "Average loss at step 1800: 7.797605 learning rate: 0.397214\n",
      "Minibatch accuracy1: 4.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 9.52%\n",
      "Average loss at step 1900: 7.504292 learning rate: 0.377353\n",
      "Minibatch accuracy1: 16.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 12.00%\n",
      "Average loss at step 2000: 7.247905 learning rate: 0.358486\n",
      "Minibatch accuracy1: 18.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 7.41%\n",
      "Average loss at step 2100: 7.123086 learning rate: 0.340562\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 14.29%\n",
      "Average loss at step 2200: 7.020969 learning rate: 0.323533\n",
      "Minibatch accuracy1: 16.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 15.00%\n",
      "Average loss at step 2300: 6.999777 learning rate: 0.307357\n",
      "Minibatch accuracy1: 4.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 8.00%\n",
      "Average loss at step 2400: 6.967731 learning rate: 0.291989\n",
      "Minibatch accuracy1: 22.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 7.14%\n",
      "Average loss at step 2500: 6.953311 learning rate: 0.277389\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 10.34%\n",
      "Average loss at step 2600: 6.944957 learning rate: 0.263520\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 15.38%\n",
      "Average loss at step 2700: 6.940301 learning rate: 0.250344\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 4.00%\n",
      "Average loss at step 2800: 6.933928 learning rate: 0.237827\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 25.00%\n",
      "Average loss at step 2900: 6.931983 learning rate: 0.225935\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 14.29%\n",
      "Average loss at step 3000: 6.927004 learning rate: 0.214639\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 12.50%\n",
      "Average loss at step 3100: 6.925202 learning rate: 0.203907\n",
      "Minibatch accuracy1: 4.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 14.81%\n",
      "Average loss at step 3200: 6.923220 learning rate: 0.193711\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 16.00%\n",
      "Minibatch accuracy3: 7.41%\n",
      "Average loss at step 3300: 6.921275 learning rate: 0.184026\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 7.69%\n",
      "Average loss at step 3400: 6.919602 learning rate: 0.174825\n",
      "Minibatch accuracy1: 16.00%\n",
      "Minibatch accuracy2: 14.00%\n",
      "Minibatch accuracy3: 13.04%\n",
      "Average loss at step 3500: 6.920040 learning rate: 0.166083\n",
      "Minibatch accuracy1: 4.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 7.69%\n",
      "Average loss at step 3600: 6.918313 learning rate: 0.157779\n",
      "Minibatch accuracy1: 14.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 3.57%\n",
      "Average loss at step 3700: 6.917195 learning rate: 0.149890\n",
      "Minibatch accuracy1: 18.00%\n",
      "Minibatch accuracy2: 12.00%\n",
      "Minibatch accuracy3: 4.00%\n",
      "Average loss at step 3800: 6.915595 learning rate: 0.142396\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 17.39%\n",
      "Average loss at step 3900: 6.916509 learning rate: 0.135276\n",
      "Minibatch accuracy1: 8.00%\n",
      "Minibatch accuracy2: 6.00%\n",
      "Minibatch accuracy3: 24.00%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    mean_loss = 0\n",
    "    for step in range(7001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]\n",
    "        _, _, _, l, predictions1, predictions2, predictions3, lr, labels1, labels2, labels3 = sess.run(\n",
    "          [optimizer1, optimizer2, optimizer3, global_loss, train_prediction1, train_prediction2, train_prediction3,\n",
    "           learning_rate,\n",
    "           tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "#             print('Minibatch accuracy_s: %.2f%%' % accuracy(predictions_s, labels_s))\n",
    "            print('Minibatch accuracy1: %.2f%%' % accuracy(predictions1, labels1))\n",
    "            print('Minibatch accuracy2: %.2f%%' % accuracy(predictions2, labels2))\n",
    "            print('Minibatch accuracy3: %.2f%%' % accuracy(predictions3, labels3))\n",
    "    saver.save(sess,\"model2/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "使用已训练的模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model2/model.ckpt\n",
      "Average loss at step 0: 6.911144 learning rate: 0.027584\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 16.67%\n",
      "Average loss at step 100: 6.908957 learning rate: 0.026204\n",
      "Minibatch accuracy1: 10.00%\n",
      "Minibatch accuracy2: 4.00%\n",
      "Minibatch accuracy3: 14.29%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess,\"model2/model.ckpt\")\n",
    "    mean_loss = 0\n",
    "#     reset_step = global_step.assign(0)\n",
    "#     reset_step.eval()\n",
    "    for step in range(101):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]\n",
    "        _, _, _, l, predictions1, predictions2, predictions3, lr, labels1, labels2, labels3 = sess.run(\n",
    "          [optimizer1, optimizer2, optimizer3, global_loss, train_prediction1, train_prediction2, train_prediction3,\n",
    "           learning_rate,\n",
    "           tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "#             print('Minibatch accuracy_s: %.2f%%' % accuracy(predictions_s, labels_s))\n",
    "            print('Minibatch accuracy1: %.2f%%' % accuracy(predictions1, labels1))\n",
    "            print('Minibatch accuracy2: %.2f%%' % accuracy(predictions2, labels2))\n",
    "            print('Minibatch accuracy3: %.2f%%' % accuracy(predictions3, labels3))\n",
    "    saver.save(sess,\"model2/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
