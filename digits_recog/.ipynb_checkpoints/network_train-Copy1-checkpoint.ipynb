{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 建立并训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from RNNCell_diy import diyLSTMCell\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pfile = './concat_image_train_data.pickle'\n",
    "with open(pfile, \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "train_dataset = train_data[\"train_dataset\"]\n",
    "train_dataset.shape = train_dataset.shape + (1,)\n",
    "train_labels = train_data[\"train_labels\"]\n",
    "train_labels_num = train_data[\"train_labels_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cdic = {\"a\":0, \"b\":1, \"c\":2, \"d\":3, \"e\":4, \"f\":5, \"g\":6, \"h\":7, \"i\":8, \"j\":9, \" \":10}\n",
    "cdic_r = {}\n",
    "for key, value in cdic.items():\n",
    "    cdic_r[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "image_size_h = 56\n",
    "image_size_w = 112\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "构建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 5, 1, 12) dtype=float32_ref>, <tf.Variable 'bias1:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(5, 5, 12, 32) dtype=float32_ref>, <tf.Variable 'bias3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(4608, 300) dtype=float32_ref>, <tf.Variable 'bias5:0' shape=(300,) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(300, 260) dtype=float32_ref>, <tf.Variable 'bias6:0' shape=(260,) dtype=float32_ref>, <tf.Variable 'Variable_4:0' shape=(260, 3) dtype=float32_ref>, <tf.Variable 's_bias:0' shape=(3,) dtype=float32_ref>, <tf.Variable 'Variable_5:0' shape=(260, 11) dtype=float32_ref>, <tf.Variable 'c1_bias:0' shape=(11,) dtype=float32_ref>, <tf.Variable 'Variable_6:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c2_bias:0' shape=(10,) dtype=float32_ref>, <tf.Variable 'Variable_7:0' shape=(260, 10) dtype=float32_ref>, <tf.Variable 'c3_bias:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "kernel_size = 5\n",
    "pooling_size = 2\n",
    "channels_one = 12\n",
    "channels_two = 32\n",
    "hidden1_size = 300\n",
    "hidden2_size = 260\n",
    "dropout = 0.5\n",
    "lam = 0.01\n",
    "max_time_step = 3\n",
    "rnn_num_nodes = 260\n",
    "rnn_num_layers = 3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size_h, image_size_w, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32,shape=(batch_size, max_time_step))\n",
    "    tf_train_labels_num = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    \n",
    "    #variables\n",
    "    layer_weight1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, num_channels, channels_one],stddev=0.1))\n",
    "    layer_biases1 = tf.Variable(tf.zeros([channels_one]), name=\"bias1\")\n",
    "    \n",
    "    layer_weight3 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels_one, channels_two], stddev=0.1))\n",
    "    layer_biases3 = tf.Variable(tf.constant(1.0, shape=[channels_two]), name=\"bias3\")\n",
    "    \n",
    "    neuron_num = ((((image_size_h-pooling_size)//pooling_size+1-kernel_size+1-pooling_size)//pooling_size+1)**2)*channels_two\n",
    "    layer_weight5 = tf.Variable(tf.truncated_normal([neuron_num, hidden1_size], stddev=0.1))\n",
    "    layer_biases5 = tf.Variable(tf.constant(1.0, shape=[hidden1_size]), name=\"bias5\")\n",
    "    \n",
    "    layer_weight6 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "    layer_biases6 = tf.Variable(tf.constant(1.0, shape=[hidden2_size]), name=\"bias6\")\n",
    "    \n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    s_weight = tf.Variable(tf.truncated_normal([hidden2_size, max_time_step], stddev=0.1))\n",
    "    s_biases = tf.Variable(tf.constant(1.0, shape=[max_time_step]), name=\"s_bias\")\n",
    "    \n",
    "    c1_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)], stddev=0.1))\n",
    "    c1_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)]), name=\"c1_bias\")\n",
    "    \n",
    "    c2_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1))\n",
    "    c2_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c2_bias\")\n",
    "    \n",
    "    c3_weight = tf.Variable(tf.truncated_normal([hidden2_size, len(cdic)-1], stddev=0.1))\n",
    "    c3_biases = tf.Variable(tf.constant(1.0, shape=[len(cdic)-1]), name=\"c3_bias\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     w = tf.Variable(tf.truncated_normal([rnn_num_nodes, len(cdic)], -0.1, 0.1))\n",
    "#     b = tf.Variable(tf.zeros([len(cdic)]), name=\"bias_o\")\n",
    "    \n",
    "    # Model\n",
    "    def conv_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer_weight1, [1,1,1,1], padding=\"SAME\")+layer_biases1\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,4,1], [1,2,4,1], padding=\"VALID\"))\n",
    "        conv = tf.nn.conv2d(pooling, layer_weight3, [1,1,1,1], padding=\"VALID\")+layer_biases3\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding=\"VALID\"))\n",
    "        shape = pooling.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling,[shape[0],shape[1]*shape[2]*shape[3]])\n",
    "        hidden1 = tf.nn.dropout(tf.matmul(reshape, layer_weight5)+layer_biases5, dropout)\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, layer_weight6)+layer_biases6)\n",
    "        return hidden2\n",
    "            \n",
    "#     def rnn_model1(input_data):\n",
    "#         cell_list = []\n",
    "#         for i in range(rnn_num_layers):\n",
    "#             rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "#             rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "#             cell_list.append(rnn_cell)\n",
    "#         rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "#         with tf.variable_scope(\"rnn1\"):\n",
    "#             rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "#                 rnn_cells,\n",
    "#                 input_data,\n",
    "#                 dtype=tf.float32)\n",
    "#         return rnn_outputs, rnn_state\n",
    "    \n",
    "#     def rnn_model2(input_data, init_state):\n",
    "#         cell_list = []\n",
    "#         for i in range(rnn_num_layers):\n",
    "#             rnn_cell = diyLSTMCell(rnn_num_nodes)\n",
    "#             rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "#             cell_list.append(rnn_cell)\n",
    "#         rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "#         with tf.variable_scope(\"rnn2\"):\n",
    "#             rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "#                 rnn_cells,\n",
    "#                 input_data,\n",
    "#                 dtype=tf.float32,\n",
    "#                 initial_state=init_state,\n",
    "#                 sequence_length=tf_train_labels_num+1,\n",
    "#                 swap_memory=True)\n",
    "#         return rnn_outputs, rnn_state\n",
    "        \n",
    "    \n",
    "    #loss\n",
    "    conv_out = conv_model(tf_train_dataset)\n",
    "#     conv_out = tf.reshape(conv_out,[batch_size,1,-1])\n",
    "#     rnn1_output, rnn1_state = rnn_model1(conv_out)\n",
    "#     input_data = tf.concat((tf.constant(cdic[\"remain\"],shape=[batch_size,1]), tf_train_labels),1)\n",
    "#     input_data = tf.one_hot(input_data,len(cdic))\n",
    "#     tf_train_labels_c = tf.concat((tf_train_labels,tf.constant(cdic[\"remain\"],shape=[batch_size,1])),1)\n",
    "#     tf_train_labels_c = tf.one_hot(tf_train_labels_c,len(cdic))\n",
    "#     rnn2_outputs, _ = rnn_model2(input_data, rnn1_state)\n",
    "#     rnn2_outputs = tf.reshape(rnn2_outputs,[-1, rnn_num_nodes])\n",
    "#     logits = tf.reshape(tf.matmul(rnn2_outputs, w) + b, [batch_size, max_time_step+1,-1])\n",
    "\n",
    "    tf_train_labels_c1 = tf_train_labels[:,0]\n",
    "    tf_train_labels_c1 = tf.one_hot(tf_train_labels_c1, len(cdic))\n",
    "    c1_logits = tf.matmul(conv_out, c1_weight)+c1_biases\n",
    "    \n",
    "    tf_train_labels_c2 = tf_train_labels[:,1]\n",
    "    mask = tf.logical_not(tf.equal(tf_train_labels_c2, cdic[\" \"])) # boolean tensor, mask[i] = True iff x[i] ==cdic[\" \"]\n",
    "    tf_train_labels_c2 = tf.boolean_mask(tf_train_labels_c2, mask)\n",
    "    tf_train_labels_c2 = tf.one_hot(tf_train_labels_c2, len(cdic)-1)\n",
    "#     c2_logits = tf.matmul(conv_out, c2_weight)+c2_biases\n",
    "    c2_logits = tf.matmul(tf.boolean_mask(conv_out, mask), c2_weight)+c2_biases\n",
    "\n",
    "    \n",
    "    tf_train_labels_c3 = tf_train_labels[:,2]\n",
    "    mask = tf.logical_not(tf.equal(tf_train_labels_c3, cdic[\" \"])) # boolean tensor, mask[i] = True iff x[i] ==cdic[\" \"]\n",
    "    tf_train_labels_c3 = tf.boolean_mask(tf_train_labels_c3, mask)\n",
    "    tf_train_labels_c3 = tf.one_hot(tf_train_labels_c3, len(cdic)-1)\n",
    "#     c3_logits = tf.matmul(conv_out, c3_weight)+c3_biases\n",
    "    c3_logits = tf.matmul(tf.boolean_mask(conv_out, mask), c3_weight)+c3_biases\n",
    "    \n",
    "    tf_train_labels_s = tf.one_hot(tf_train_labels_num, max_time_step)\n",
    "    s_logits = tf.matmul(conv_out, s_weight)+s_biases\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if not(\"bias\" in v.name)])\n",
    "    \n",
    "    mu3 = tf.cast(tf.shape(tf_train_labels_c3)[0], tf.float32)/batch_size\n",
    "\n",
    "\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c1, logits=c1_logits)) \\\n",
    "        +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c2, logits=c2_logits)) \\\n",
    "        +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c3, logits=c3_logits)) \\\n",
    "        +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_s, logits=s_logits)) \\\n",
    "        +lam*regularization_cost\n",
    "        \n",
    "#     loss = mu3*(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c3, logits=c3_logits))+lam*regularization_cost)\n",
    "#     loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c2, logits=c2_logits)) \\\n",
    "#         +tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_s, logits=s_logits)) \\\n",
    "#         +lam*regularization_cost\n",
    "    \n",
    "    # Optimizer.\n",
    "#     global_step = tf.Variable(0)\n",
    "#     learning_rate = tf.train.exponential_decay(\n",
    "#       10.0, global_step, 100, 0.95, staircase=True)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#     gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "#     gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "#     optimizer = optimizer.apply_gradients(\n",
    "#       zip(gradients, v), global_step=global_step)\n",
    "    learning_rate = tf.constant(1e-7)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = optimizer.minimize(loss)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction1 = tf.nn.softmax(c1_logits)\n",
    "    train_prediction2 = tf.nn.softmax(c2_logits)\n",
    "    train_prediction3 = tf.nn.softmax(c3_logits)\n",
    "    train_prediction_s = tf.nn.softmax(s_logits)\n",
    "    \n",
    "    #model save\n",
    "    tv = tf.trainable_variables()\n",
    "    print(tv)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "第一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 142.443619 learning rate: 0.000100\n",
      "Minibatch accuracy_s: 14.00%\n",
      "Minibatch accuracy1: 12.00%\n",
      "Minibatch accuracy2: 8.00%\n",
      "Minibatch accuracy3: 10.71%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-62dce3980a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m           [optimizer, loss, train_prediction1, train_prediction2, train_prediction3, train_prediction_s,\n\u001b[1;32m     12\u001b[0m            \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m            tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3, tf_train_labels_s], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    mean_loss = 0\n",
    "    for step in range(7001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]-1\n",
    "        _,l, predictions1, predictions2, predictions3, predictions_s, lr, labels1, labels2, labels3, labels_s = sess.run(\n",
    "          [optimizer, loss, train_prediction1, train_prediction2, train_prediction3, train_prediction_s,\n",
    "           learning_rate,\n",
    "           tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3, tf_train_labels_s], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch accuracy_s: %.2f%%' % accuracy(predictions_s, labels_s))\n",
    "            print('Minibatch accuracy1: %.2f%%' % accuracy(predictions1, labels1))\n",
    "            print('Minibatch accuracy2: %.2f%%' % accuracy(predictions2, labels2))\n",
    "            print('Minibatch accuracy3: %.2f%%' % accuracy(predictions3, labels3))\n",
    "    saver.save(sess,\"model2/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "使用已训练的模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model2/model.ckpt\n",
      "Average loss at step 0: 2.651942 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 72.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 89.66%\n",
      "Average loss at step 100: 2.624721 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 88.89%\n",
      "Average loss at step 200: 2.650823 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 84.00%\n",
      "Minibatch accuracy2: 90.00%\n",
      "Minibatch accuracy3: 80.00%\n",
      "Average loss at step 300: 2.651330 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 80.00%\n",
      "Minibatch accuracy3: 93.33%\n",
      "Average loss at step 400: 2.625053 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 92.59%\n",
      "Average loss at step 500: 2.617458 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 70.00%\n",
      "Minibatch accuracy2: 88.00%\n",
      "Minibatch accuracy3: 88.89%\n",
      "Average loss at step 600: 2.647692 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 76.00%\n",
      "Minibatch accuracy2: 76.00%\n",
      "Minibatch accuracy3: 84.62%\n",
      "Average loss at step 700: 2.641912 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 78.00%\n",
      "Minibatch accuracy2: 84.00%\n",
      "Minibatch accuracy3: 88.00%\n",
      "Average loss at step 800: 2.620635 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 90.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 90.62%\n",
      "Average loss at step 900: 2.684419 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 74.00%\n",
      "Minibatch accuracy2: 90.00%\n",
      "Minibatch accuracy3: 80.77%\n",
      "Average loss at step 1000: 2.640771 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 90.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 91.67%\n",
      "Average loss at step 1100: 2.656918 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 78.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 84.38%\n",
      "Average loss at step 1200: 2.678558 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 70.00%\n",
      "Minibatch accuracy2: 84.00%\n",
      "Minibatch accuracy3: 72.41%\n",
      "Average loss at step 1300: 2.649913 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 88.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 95.65%\n",
      "Average loss at step 1400: 2.655237 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 82.00%\n",
      "Minibatch accuracy2: 80.00%\n",
      "Minibatch accuracy3: 82.61%\n",
      "Average loss at step 1500: 2.625476 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 78.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 95.45%\n",
      "Average loss at step 1600: 2.635370 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 76.00%\n",
      "Minibatch accuracy2: 94.00%\n",
      "Minibatch accuracy3: 83.33%\n",
      "Average loss at step 1700: 2.683641 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 80.00%\n",
      "Minibatch accuracy2: 86.00%\n",
      "Minibatch accuracy3: 77.78%\n",
      "Average loss at step 1800: 2.636013 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 98.00%\n",
      "Minibatch accuracy1: 72.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 85.71%\n",
      "Average loss at step 1900: 2.661574 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 80.00%\n",
      "Minibatch accuracy2: 88.00%\n",
      "Minibatch accuracy3: 70.83%\n",
      "Average loss at step 2000: 2.623312 learning rate: 0.000000\n",
      "Minibatch accuracy_s: 100.00%\n",
      "Minibatch accuracy1: 84.00%\n",
      "Minibatch accuracy2: 82.00%\n",
      "Minibatch accuracy3: 80.00%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess,\"model2/model.ckpt\")\n",
    "    mean_loss = 0\n",
    "#     reset_step = global_step.assign(500)\n",
    "#     reset_step.eval()\n",
    "    for step in range(2001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]-1\n",
    "        _,l, predictions1, predictions2, predictions3, predictions_s, lr, labels1, labels2, labels3, labels_s = sess.run(\n",
    "          [optimizer, loss, train_prediction1, train_prediction2, train_prediction3, train_prediction_s,\n",
    "           learning_rate,\n",
    "           tf_train_labels_c1, tf_train_labels_c2, tf_train_labels_c3, tf_train_labels_s], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %.10f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch accuracy_s: %.2f%%' % accuracy(predictions_s, labels_s))\n",
    "            print('Minibatch accuracy1: %.2f%%' % accuracy(predictions1, labels1))\n",
    "            print('Minibatch accuracy2: %.2f%%' % accuracy(predictions2, labels2))\n",
    "            print('Minibatch accuracy3: %.2f%%' % accuracy(predictions3, labels3))\n",
    "    saver.save(sess,\"model2/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
