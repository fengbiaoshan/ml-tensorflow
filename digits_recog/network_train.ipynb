{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立并训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from RNNCell_diy import diyLSTMCell\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pfile = './concat_image_train_data.pickle'\n",
    "with open(pfile, \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "train_dataset = train_data[\"train_dataset\"]\n",
    "train_dataset.shape = train_dataset.shape + (1,)\n",
    "train_labels = train_data[\"train_labels\"]\n",
    "train_labels_num = train_data[\"train_labels_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdic = {\"a\":0, \"b\":1, \"c\":2, \"d\":3, \"e\":4, \"f\":5, \"g\":6, \"h\":7, \"i\":8, \"j\":9, \"remain\":10}\n",
    "cdic_r = {}\n",
    "for key, value in cdic.items():\n",
    "    cdic_r[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size_h = 56\n",
    "image_size_w = 112\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 5, 1, 12) dtype=float32_ref>, <tf.Variable 'bias1:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'Variable_1:0' shape=(5, 5, 12, 32) dtype=float32_ref>, <tf.Variable 'bias3:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Variable_2:0' shape=(4608, 300) dtype=float32_ref>, <tf.Variable 'bias5:0' shape=(300,) dtype=float32_ref>, <tf.Variable 'Variable_3:0' shape=(300, 260) dtype=float32_ref>, <tf.Variable 'bias6:0' shape=(260,) dtype=float32_ref>, <tf.Variable 'Variable_4:0' shape=(260, 11) dtype=float32_ref>, <tf.Variable 'bias_o:0' shape=(11,) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn1/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(271, 1040) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0' shape=(520, 1040) dtype=float32_ref>, <tf.Variable 'rnn2/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0' shape=(1040,) dtype=float32_ref>, <tf.Variable 'Variable_5:0' shape=() dtype=int32_ref>]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "kernel_size = 5\n",
    "pooling_size = 2\n",
    "channels_one = 12\n",
    "channels_two = 32\n",
    "hidden1_size = 300\n",
    "hidden2_size = 260\n",
    "dropout = 0.5\n",
    "lam = 0.01\n",
    "max_time_step = 3\n",
    "rnn_num_nodes = 260\n",
    "rnn_num_layers = 3\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #input data\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size_h, image_size_w, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.int32,shape=(batch_size, max_time_step))\n",
    "    tf_train_labels_num = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    \n",
    "    #variables\n",
    "    layer_weight1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, num_channels, channels_one],stddev=0.1))\n",
    "    layer_biases1 = tf.Variable(tf.zeros([channels_one]), name=\"bias1\")\n",
    "    \n",
    "    layer_weight3 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels_one, channels_two], stddev=0.1))\n",
    "    layer_biases3 = tf.Variable(tf.constant(1.0, shape=[channels_two]), name=\"bias3\")\n",
    "    \n",
    "    neuron_num = ((((image_size_h-pooling_size)//pooling_size+1-kernel_size+1-pooling_size)//pooling_size+1)**2)*channels_two\n",
    "    layer_weight5 = tf.Variable(tf.truncated_normal([neuron_num, hidden1_size], stddev=0.1))\n",
    "    layer_biases5 = tf.Variable(tf.constant(1.0, shape=[hidden1_size]), name=\"bias5\")\n",
    "    \n",
    "    layer_weight6 = tf.Variable(tf.truncated_normal([hidden1_size, hidden2_size], stddev=0.1))\n",
    "    layer_biases6 = tf.Variable(tf.constant(1.0, shape=[hidden2_size]), name=\"bias6\")\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([rnn_num_nodes, len(cdic)], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([len(cdic)]), name=\"bias_o\")\n",
    "    \n",
    "    # Model\n",
    "    def conv_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer_weight1, [1,1,1,1], padding=\"SAME\")+layer_biases1\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,4,1], [1,2,4,1], padding=\"VALID\"))\n",
    "        conv = tf.nn.conv2d(pooling, layer_weight3, [1,1,1,1], padding=\"VALID\")+layer_biases3\n",
    "        pooling = tf.nn.relu(tf.nn.max_pool(conv, [1,2,2,1], [1,2,2,1], padding=\"VALID\"))\n",
    "        shape = pooling.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling,[shape[0],shape[1]*shape[2]*shape[3]])\n",
    "        hidden1 = tf.nn.dropout(tf.matmul(reshape, layer_weight5)+layer_biases5, dropout)\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, layer_weight6)+layer_biases6)\n",
    "        return hidden2\n",
    "            \n",
    "    def rnn_model1(input_data):\n",
    "        cell_list = []\n",
    "        for i in range(rnn_num_layers):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_num_nodes)\n",
    "            rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "            cell_list.append(rnn_cell)\n",
    "        rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        with tf.variable_scope(\"rnn1\"):\n",
    "            rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "                rnn_cells,\n",
    "                input_data,\n",
    "                dtype=tf.float32)\n",
    "        return rnn_outputs, rnn_state\n",
    "    \n",
    "    def rnn_model2(input_data, init_state):\n",
    "        cell_list = []\n",
    "        for i in range(rnn_num_layers):\n",
    "            rnn_cell = tf.contrib.rnn.BasicLSTMCell(rnn_num_nodes)\n",
    "            rnn_cell = tf.contrib.rnn.DropoutWrapper(cell=rnn_cell, input_keep_prob=(1.0 - dropout))\n",
    "            cell_list.append(rnn_cell)\n",
    "        rnn_cells = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        with tf.variable_scope(\"rnn2\"):\n",
    "            rnn_outputs, rnn_state = tf.nn.dynamic_rnn(\n",
    "                rnn_cells,\n",
    "                input_data,\n",
    "                dtype=tf.float32,\n",
    "                initial_state=init_state,\n",
    "                sequence_length=tf_train_labels_num+1,\n",
    "                swap_memory=True)\n",
    "        return rnn_outputs, rnn_state\n",
    "        \n",
    "    \n",
    "    #loss\n",
    "    conv_out = conv_model(tf_train_dataset)\n",
    "    conv_out = tf.reshape(conv_out,[batch_size,1,-1])\n",
    "    rnn1_output, rnn1_state = rnn_model1(conv_out)\n",
    "    input_data = tf.concat((tf.constant(cdic[\"remain\"],shape=[batch_size,1]), tf_train_labels),1)\n",
    "    input_data = tf.one_hot(input_data,len(cdic))\n",
    "    tf_train_labels_c = tf.concat((tf_train_labels,tf.constant(cdic[\"remain\"],shape=[batch_size,1])),1)\n",
    "    tf_train_labels_c = tf.reshape(tf.transpose(tf_train_labels_c), [-1])\n",
    "    tf_train_labels_c = tf.one_hot(tf_train_labels_c,len(cdic))\n",
    "    rnn2_outputs, _ = rnn_model2(input_data, rnn1_state)\n",
    "    rnn2_outputs = tf.transpose(rnn2_outputs, [1,0,2])\n",
    "    rnn2_outputs = tf.reshape(rnn2_outputs,[-1, rnn_num_nodes])\n",
    "    logits = tf.matmul(rnn2_outputs, w) + b\n",
    "    \n",
    "    tv = tf.trainable_variables()\n",
    "    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv if not(\"bias\" in v.name)])\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels_c, logits=logits))+lam*regularization_cost\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "      1.0, global_step, 100, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "      zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    #model save\n",
    "    tv = tf.trainable_variables()\n",
    "    print(tv)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 0: 79.039337 learning rate: 1.000000\n",
      "Minibatch perplexity: 11.00\n",
      "Average loss at step 100: 34.730603 learning rate: 0.950000\n",
      "Minibatch perplexity: 5.95\n",
      "Average loss at step 200: 6.387562 learning rate: 0.902500\n",
      "Minibatch perplexity: 5.72\n",
      "Average loss at step 300: 2.497618 learning rate: 0.857375\n",
      "Minibatch perplexity: 5.87\n",
      "Average loss at step 400: 1.910515 learning rate: 0.814506\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 500: 1.793289 learning rate: 0.773781\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 600: 1.779321 learning rate: 0.735092\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 700: 1.756707 learning rate: 0.698337\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 800: 1.754028 learning rate: 0.663420\n",
      "Minibatch perplexity: 5.27\n",
      "Average loss at step 900: 1.741110 learning rate: 0.630249\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 1000: 1.741074 learning rate: 0.598737\n",
      "Minibatch perplexity: 5.20\n",
      "Average loss at step 1100: 1.735081 learning rate: 0.568800\n",
      "Minibatch perplexity: 5.16\n",
      "Average loss at step 1200: 1.738184 learning rate: 0.540360\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 1300: 1.724805 learning rate: 0.513342\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 1400: 1.720590 learning rate: 0.487675\n",
      "Minibatch perplexity: 5.20\n",
      "Average loss at step 1500: 1.724199 learning rate: 0.463291\n",
      "Minibatch perplexity: 5.36\n",
      "Average loss at step 1600: 1.718979 learning rate: 0.440127\n",
      "Minibatch perplexity: 5.62\n",
      "Average loss at step 1700: 1.720641 learning rate: 0.418120\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 1800: 1.714176 learning rate: 0.397214\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 1900: 1.712965 learning rate: 0.377353\n",
      "Minibatch perplexity: 5.01\n",
      "Average loss at step 2000: 1.711248 learning rate: 0.358486\n",
      "Minibatch perplexity: 5.10\n",
      "Average loss at step 2100: 1.705438 learning rate: 0.340562\n",
      "Minibatch perplexity: 5.03\n",
      "Average loss at step 2200: 1.705917 learning rate: 0.323533\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 2300: 1.697965 learning rate: 0.307357\n",
      "Minibatch perplexity: 5.29\n",
      "Average loss at step 2400: 1.696181 learning rate: 0.291989\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 2500: 1.693287 learning rate: 0.277389\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 2600: 1.700556 learning rate: 0.263520\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 2700: 1.695664 learning rate: 0.250344\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 2800: 1.692158 learning rate: 0.237827\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 2900: 1.693542 learning rate: 0.225935\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 3000: 1.690087 learning rate: 0.214639\n",
      "Minibatch perplexity: 5.12\n",
      "Average loss at step 3100: 1.694936 learning rate: 0.203907\n",
      "Minibatch perplexity: 5.36\n",
      "Average loss at step 3200: 1.697992 learning rate: 0.193711\n",
      "Minibatch perplexity: 5.32\n",
      "Average loss at step 3300: 1.687434 learning rate: 0.184026\n",
      "Minibatch perplexity: 4.72\n",
      "Average loss at step 3400: 1.689806 learning rate: 0.174825\n",
      "Minibatch perplexity: 5.13\n",
      "Average loss at step 3500: 1.688580 learning rate: 0.166083\n",
      "Minibatch perplexity: 4.82\n",
      "Average loss at step 3600: 1.687931 learning rate: 0.157779\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 3700: 1.693220 learning rate: 0.149890\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 3800: 1.679058 learning rate: 0.142396\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 3900: 1.685591 learning rate: 0.135276\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 4000: 1.684733 learning rate: 0.128512\n",
      "Minibatch perplexity: 5.13\n",
      "Average loss at step 4100: 1.676364 learning rate: 0.122086\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 4200: 1.681642 learning rate: 0.115982\n",
      "Minibatch perplexity: 5.24\n",
      "Average loss at step 4300: 1.685435 learning rate: 0.110183\n",
      "Minibatch perplexity: 4.87\n",
      "Average loss at step 4400: 1.676817 learning rate: 0.104674\n",
      "Minibatch perplexity: 5.12\n",
      "Average loss at step 4500: 1.684315 learning rate: 0.099440\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 4600: 1.682878 learning rate: 0.094468\n",
      "Minibatch perplexity: 5.02\n",
      "Average loss at step 4700: 1.687124 learning rate: 0.089745\n",
      "Minibatch perplexity: 4.81\n",
      "Average loss at step 4800: 1.670705 learning rate: 0.085258\n",
      "Minibatch perplexity: 4.83\n",
      "Average loss at step 4900: 1.682967 learning rate: 0.080995\n",
      "Minibatch perplexity: 5.42\n",
      "Average loss at step 5000: 1.680421 learning rate: 0.076945\n",
      "Minibatch perplexity: 4.64\n",
      "Average loss at step 5100: 1.683859 learning rate: 0.073098\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 5200: 1.679209 learning rate: 0.069443\n",
      "Minibatch perplexity: 5.13\n",
      "Average loss at step 5300: 1.668279 learning rate: 0.065971\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 5400: 1.678700 learning rate: 0.062672\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 5500: 1.676319 learning rate: 0.059539\n",
      "Minibatch perplexity: 5.31\n",
      "Average loss at step 5600: 1.683490 learning rate: 0.056562\n",
      "Minibatch perplexity: 5.45\n",
      "Average loss at step 5700: 1.678923 learning rate: 0.053733\n",
      "Minibatch perplexity: 5.23\n",
      "Average loss at step 5800: 1.678382 learning rate: 0.051047\n",
      "Minibatch perplexity: 5.08\n",
      "Average loss at step 5900: 1.673644 learning rate: 0.048494\n",
      "Minibatch perplexity: 5.32\n",
      "Average loss at step 6000: 1.673029 learning rate: 0.046070\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 6100: 1.673982 learning rate: 0.043766\n",
      "Minibatch perplexity: 5.18\n",
      "Average loss at step 6200: 1.677743 learning rate: 0.041578\n",
      "Minibatch perplexity: 5.42\n",
      "Average loss at step 6300: 1.675114 learning rate: 0.039499\n",
      "Minibatch perplexity: 5.00\n",
      "Average loss at step 6400: 1.678246 learning rate: 0.037524\n",
      "Minibatch perplexity: 5.06\n",
      "Average loss at step 6500: 1.684525 learning rate: 0.035648\n",
      "Minibatch perplexity: 4.74\n",
      "Average loss at step 6600: 1.684921 learning rate: 0.033866\n",
      "Minibatch perplexity: 5.10\n",
      "Average loss at step 6700: 1.672291 learning rate: 0.032172\n",
      "Minibatch perplexity: 5.24\n",
      "Average loss at step 6800: 1.671718 learning rate: 0.030564\n",
      "Minibatch perplexity: 4.93\n",
      "Average loss at step 6900: 1.682377 learning rate: 0.029035\n",
      "Minibatch perplexity: 5.32\n",
      "Average loss at step 7000: 1.676975 learning rate: 0.027584\n",
      "Minibatch perplexity: 5.28\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    mean_loss = 0\n",
    "    for step in range(7001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]\n",
    "        _,l, predictions, lr, labels = sess.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate, tf_train_labels_c], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    saver.save(sess,\"model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用已训练的模型继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "Average loss at step 0: 1.634242 learning rate: 0.009888\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 100: 1.679172 learning rate: 0.009394\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 200: 1.671983 learning rate: 0.008924\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 300: 1.679619 learning rate: 0.008478\n",
      "Minibatch perplexity: 4.88\n",
      "Average loss at step 400: 1.677630 learning rate: 0.008054\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 500: 1.681681 learning rate: 0.007651\n",
      "Minibatch perplexity: 5.31\n",
      "Average loss at step 600: 1.668924 learning rate: 0.007269\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 700: 1.678885 learning rate: 0.006905\n",
      "Minibatch perplexity: 5.31\n",
      "Average loss at step 800: 1.675388 learning rate: 0.006560\n",
      "Minibatch perplexity: 5.10\n",
      "Average loss at step 900: 1.671372 learning rate: 0.006232\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 1000: 1.670407 learning rate: 0.005921\n",
      "Minibatch perplexity: 4.60\n",
      "Average loss at step 1100: 1.683492 learning rate: 0.005624\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 1200: 1.672942 learning rate: 0.005343\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 1300: 1.677182 learning rate: 0.005076\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 1400: 1.677896 learning rate: 0.004822\n",
      "Minibatch perplexity: 4.72\n",
      "Average loss at step 1500: 1.672269 learning rate: 0.004581\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 1600: 1.664734 learning rate: 0.004352\n",
      "Minibatch perplexity: 5.04\n",
      "Average loss at step 1700: 1.675728 learning rate: 0.004135\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 1800: 1.676127 learning rate: 0.003928\n",
      "Minibatch perplexity: 5.39\n",
      "Average loss at step 1900: 1.674754 learning rate: 0.003731\n",
      "Minibatch perplexity: 5.20\n",
      "Average loss at step 2000: 1.673585 learning rate: 0.003545\n",
      "Minibatch perplexity: 4.82\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess,\"model/model.ckpt\")\n",
    "    mean_loss = 0\n",
    "#     reset_step = global_step.assign(0)\n",
    "#     reset_step.eval()\n",
    "    for step in range(2001):\n",
    "        feed_dict = {}\n",
    "        batch_index = np.random.randint(0,len(train_dataset), batch_size)\n",
    "        feed_dict[tf_train_dataset] = train_dataset[batch_index]\n",
    "        feed_dict[tf_train_labels] = train_labels[batch_index]\n",
    "        feed_dict[tf_train_labels_num] = train_labels_num[batch_index]\n",
    "        _,l, predictions, lr, labels = sess.run(\n",
    "          [optimizer, loss, train_prediction, learning_rate, tf_train_labels_c], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % 100 == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / 100\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "            'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "    saver.save(sess,\"model/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
