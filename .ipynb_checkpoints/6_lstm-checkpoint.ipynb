{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n",
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296559 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "nsct hehrxlniitcxtb rpoc z tnasgn avjoao ularsr  dscor  aympsifougnydn ybw meu o\n",
      "tpizqoorlxarvrimaekba ev  eramti fn  sl nkoq rxs tkf bngfqd  uxi fdxhjhsn m ea x\n",
      "myt iszthmqjxsnrpdommwxrno jox iroakkr v  g oypuu dimdkk i x awqy vm jaiixovnbru\n",
      "jiahpb e fu kd tl etchrnrffmzrvfnijrghglkdw qapkrkhhfxoo ka earrmssouigr  lma c \n",
      "cbyyndhgmaa  e hs dhemk qnsm   ejnsnxiyamasrcuod thwirmdhxtestbc bckycknss yo sb\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.603292 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.87\n",
      "Validation set perplexity: 10.51\n",
      "Average loss at step 200: 2.249561 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.77\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 300: 2.098811 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 8.03\n",
      "Average loss at step 400: 1.993997 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 500: 1.931418 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 600: 1.903311 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 700: 1.857198 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.815358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 900: 1.821923 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1000: 1.819205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "k as aroen leve he setore file five enru in eprimal is in bianly teinse was turn\n",
      "rued as weigh two hind eth recumwion the amiliem reare lotia te dionian parlal p\n",
      "osie cheas the isteay woll in furlism fer the relovels in red of lifo two zero h\n",
      "uls tray qoveroca prode hobe laf in the ristory the relomit in chely presied a f\n",
      "coctive verose acrive exaldes in for cenalitat and conformationalicg when are as\n",
      "================================================================================\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1100: 1.772079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1200: 1.743507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.727946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1400: 1.739777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1500: 1.727072 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.736022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1700: 1.705315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1800: 1.669895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.639266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2000: 1.693877 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "re caping to the simber futhem the pixaics roomon pen live atencruper diging int\n",
      "jective langualcown for orm spegi is chirps combunies worl worlss wall and in th\n",
      "p inolal litzes the norkenas to xopn an madal trukhson witrias ount uning the ne\n",
      "onations syspee inaics gowers evilical mani texagu lease s beass n froxt the ref\n",
      "enty was dgcsing ne four d one each kemin persaid andsficonescy three aldonatian\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.679680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2200: 1.676824 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2300: 1.639001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2400: 1.657901 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2500: 1.673220 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2600: 1.652778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2700: 1.655708 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2800: 1.651526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2900: 1.648636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3000: 1.649250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "s tramesmor wragge the spandisp his infland to the um seven sinke grance sp noks\n",
      "mad of neadition and was tofely have nocies accaristly taught was neargh cales t\n",
      "deratiked basis us convents in one nine seven bjone na form persosemaus an airet\n",
      "istic other and way countion given gattle a tralital popularists reall foreit ob\n",
      "mbald and was dividally and francitical s well play banday hetther from changed \n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100: 1.628534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3200: 1.645809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3300: 1.635472 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3400: 1.667806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.658023 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3600: 1.667921 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3700: 1.642629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.645353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3900: 1.642501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4000: 1.653841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "ur iverented other that gevers cormon after facts defferents world aboutgers in \n",
      "x the facce the perif and brities the ciocarus in the hiscan histores with he re\n",
      "s berobruphes in the approbdtent four one four ghoyly blacku americanzeyer ele e\n",
      "zer caseed irist crects at intereladels the newly one nine nine n exockmates got\n",
      "y or disciest comede national it hol with suugese a west mores in genyn to capau\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4100: 1.636227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.637487 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4300: 1.618561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4400: 1.609475 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4500: 1.616978 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4600: 1.613915 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4700: 1.624189 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.631370 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.634218 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5000: 1.607790 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "================================================================================\n",
      "ule day west through have sachine mount gyadiesh no s funced in the not to presi\n",
      "hes fulls onnued was goons ialfbars usan let gulinads post four one five hinde o\n",
      "t offact thak showe say by a firsh reash with sourle severan mehned of mutminist\n",
      "g grav especieng as which a planks in one eight one seven one in him seplied on \n",
      "wn destrictic celly enger of ofticted b ories descripon one nine zero banking by\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.607454 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5200: 1.592641 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5300: 1.578291 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5400: 1.574943 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5500: 1.562464 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5600: 1.579608 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5700: 1.569392 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5800: 1.579559 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5900: 1.572792 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6000: 1.543678 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "z orizan red canneve custuage of the janast between i not spouth camsal in econo\n",
      "purity is cankan the spris as judieign atold phil jolizahy plann indesp ill of t\n",
      "y the reborn the name numer often take from supremics the koldranal bitled in th\n",
      "s deets and hong his six contains havi provens to kana and was any many to hidja\n",
      "uthment conquently midu lassul not seas scandfor orther and the respredces revis\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6100: 1.563872 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.534235 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6300: 1.544494 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6400: 1.538615 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6500: 1.559417 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6600: 1.597454 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6700: 1.582145 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.605123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6900: 1.582631 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 7000: 1.577791 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "y seven die basically of the port boerostally cersitows in revelop poled meandy \n",
      "ing the smarnish ligates generation produerel currenturs tackews of azsostand th\n",
      "ed onderands regues of the nated of who dary maineament tougnous wood throuth sy\n",
      "ge to timia pechoes long is the regornits battle of wargs and math six three gid\n",
      "z adome the the two zero zero foonx a leodumated to hanarn in reteent was excsum\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    cmatx = tf.matmul(i, x)\n",
    "    cmatm = tf.matmul(o, m)\n",
    "    input_gate = tf.sigmoid(cmatx[:,:num_nodes] + cmatm[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(cmatx[:,num_nodes:num_nodes*2] + cmatm[:,num_nodes:num_nodes*2] + fb)\n",
    "    update = cmatx[:,num_nodes*2:num_nodes*3] + cmatm[:,num_nodes*2:num_nodes*3] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(cmatx[:,num_nodes*3:] + cmatm[:,num_nodes*3:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296777 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      " isfu bcrwjog e zhqfeqlaewuywgjdngmn ud zrc n zowpr ektxq  uakjf maifbkyxzmvhw t\n",
      "onbarmrmthbszqaobmaqruqiiptuamit wd esgwyclkaomoknr vzlagaueqhaenltxyxfkl hzunsk\n",
      " vyoospqs cksotbbiaogq i bv zztodqapeubi koanyeeaypbj dvu  lwoivseegnoyoleksfium\n",
      "blrnfknlkynzhtmnha hwus thgrv f vej mt vodgfqkotdeew m attlkvagvtnhpwauvyvdmnsur\n",
      "jphaehao h  quu bwwat e nb zstuhw eimhizhojldii iwiwyfqp lwna imtiukk twenpio es\n",
      "================================================================================\n",
      "Validation set perplexity: 20.24\n",
      "Average loss at step 100: 2.584849 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.20\n",
      "Validation set perplexity: 10.46\n",
      "Average loss at step 200: 2.241609 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.38\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 300: 2.094171 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 400: 2.005233 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.943227 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.917055 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 700: 1.867394 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 800: 1.825284 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 900: 1.835925 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.829422 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "s cosevive chrociop ffue milic one one nine eight four bying now of to shurnaba \n",
      "x peneral lesix selely in the mayation of fochsple it groutmects regikited horau\n",
      "gupzous as of itter a centore ind mose frie a greey degate wride currfticist for\n",
      "wer ageen the stency or be aress in seven ever diget instgroccan the res befist \n",
      "fle as of in the norily corcan and spe the saxm aguramulity under expares he zel\n",
      "================================================================================\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1100: 1.778779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.755683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.737394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.750967 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1500: 1.743763 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1600: 1.746999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.716857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1800: 1.681110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900: 1.648986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2000: 1.698433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "aston davines represignzed in a pine one one nine and was in count slievle in th\n",
      "ters to ail has flosed antimpyt is was azy is engiloss for mady is formbage are \n",
      "qualla d insen in invoiving was i lephors hesess in obishn grew goss tebrids the\n",
      "qual jahis and readized sacke ro dannies be squzdoss seads one zero four linia o\n",
      " to additction capiosuenter to kirm vs marrics and also depeif refentions of sec\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2100: 1.684346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2200: 1.678350 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2300: 1.639988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2400: 1.661993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2500: 1.682033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2600: 1.653135 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.654573 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2800: 1.649603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 2900: 1.649938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3000: 1.650979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "zers even price coapsine evenal alsounded wioks gundly and state teles one nine \n",
      "dicsish office could catholicg after nighet footh in has coloupsed footwor in on\n",
      "ger was from eight cad charring one nine nine nine five with to which invalsiv i\n",
      "zer perpey blade bitth per soke vite of firusing garch aserse r of of the quacfi\n",
      "qual attlemsed and s gleagern less left evood accrers in it impoite manlarin fou\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3100: 1.629152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.643106 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3300: 1.635475 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 3400: 1.669549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3500: 1.655787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3600: 1.666008 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3700: 1.643174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.646111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3900: 1.632128 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4000: 1.650794 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "al bepended biz mones actuons edest eight is for ame he penfort search pattic we\n",
      "phoda signed for faysing the works beinalmy bly in jebrints the his spotemered t\n",
      " incocting referencts yeer holl cutty albuinca phoroken hy the goposoouts he mem\n",
      "jecke yearnolas non ru of the mirtor offented of flawhay and skiro only versemed\n",
      "ted he beholush british cabartence withs hespaenes bermm mama in the prila years\n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4100: 1.629570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4200: 1.638441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4300: 1.611103 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4400: 1.611573 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4500: 1.615129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4600: 1.620021 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4700: 1.623996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4800: 1.627019 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4900: 1.632689 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5000: 1.602769 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "================================================================================\n",
      "ons hymamiland that wis of consideration is its a s not merding most wants propa\n",
      "jucal most was demicimes with people combaye roman english that gooming as the t\n",
      " one nine sixen one zero zero zero zero peocted unto a bely one nine two ore the\n",
      "cepher birthnel one zero zero five kidd denopmine it or a jamed according treash\n",
      "met bomis two five four zero nine six more generest irversing from a local which\n",
      "================================================================================\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5100: 1.604356 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5200: 1.589217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5300: 1.573634 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5400: 1.580154 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5500: 1.565074 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5600: 1.578707 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5700: 1.564889 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5800: 1.576864 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5900: 1.573685 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.546027 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "x redamant comeition in a service harddes revieeted through american to since on\n",
      "untam to grewtrern alymakizer that the recompeted apperston high pagations the w\n",
      "resepally must acadultination early of facided the pirsed islame minnrimud in th\n",
      "viesolle are work of later and faces two zero zero fixer stays in seven nine nin\n",
      "bel apbar callency i hisson wass form fighresa is gembi subrict in the compectar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6100: 1.560143 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.532759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6300: 1.541999 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6400: 1.534267 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6500: 1.553248 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.589565 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.578068 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.601532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6900: 1.574154 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.569412 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "ger in exaborm a metical and would in parned not ryssist some monmination in the\n",
      " the mart pryfert of the contrature panace are proxime and over seven eight seve\n",
      "ons of american event using to search skitaprien later contilli capnings gnown t\n",
      "magully in sion recoventing the no prigation velly of nnotheriated to small in t\n",
      "ingh he title commolical a commond and bind are first consited funcil on the clo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary():\n",
    "    bigramdict = {}\n",
    "    re_bigramdict = {}\n",
    "    astart = ord('a')\n",
    "    count = 0\n",
    "    for i in range(27):\n",
    "        for j in range(27):\n",
    "            first = chr(astart+i) if i != 26 else \" \"\n",
    "            second = chr(astart+j) if j != 26 else \" \"\n",
    "            bigramdict[first+second] = count\n",
    "            re_bigramdict[count] = first+second\n",
    "            count += 1\n",
    "    return bigramdict, re_bigramdict\n",
    "\n",
    "bigramdict, re_bigramdict = build_dictionary()\n",
    "vocabulary_size = 27*27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b] = bigramdict[self._text[self._cursor[b]:self._cursor[b]+2]]\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [re_bigramdict[c] for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, [re_bigramdict[j] for j in b])]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # embedding vector\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))  \n",
    "  #integration matrix\n",
    "  x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes*4], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    cmatx = tf.nn.dropout(tf.matmul(i, x), 0.5)\n",
    "    cmatm = tf.matmul(o, m)\n",
    "    input_gate = tf.sigmoid(cmatx[:,:num_nodes] + cmatm[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(cmatx[:,num_nodes:num_nodes*2] + cmatm[:,num_nodes:num_nodes*2] + fb)\n",
    "    update = cmatx[:,num_nodes*2:num_nodes*3] + cmatm[:,num_nodes*2:num_nodes*3] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(cmatx[:,num_nodes*3:] + cmatm[:,num_nodes*3:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = []\n",
    "  for _ in range(num_unrollings):\n",
    "    train_inputs.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "  \n",
    "  train_labels = []  \n",
    "  for _ in range(num_unrollings):\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.int32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "  embeds = [tf.nn.embedding_lookup(embeddings, train_i) for train_i in train_inputs]\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in embeds:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.593792 learning rate: 10.000000\n",
      "Minibatch perplexity: 730.55\n",
      "================================================================================\n",
      "agdkonoswlxtczmcmmaptyyvasgnihujjefkygd nvagk axafzvcpjzrcgvied mdbmxrab y axrwvhgys  rhjqduiutacnpskutgvzptzx tbcvrnnzfkzgcrdjezokujhiopfnzwovlfvm ekgbqwsm idd\n",
      "tlkrbewdfpcuayhoiupawxdwbhztzbltsu iunjjpscyiw bzytixvffiddcgfytcmvcslvaynubyyqxvyoc qresxzekcsjteghlfqxlgjgiloxrlyncdjyjr vzovupgpoapwxtpufowkupaf dufmpuv ude \n",
      "dphtmvwrqqmzmr qtobjnvgikwrozfbfdjdmonkg qnc orpzxviizpwfuqszi cbonxaqjtwsgelgpiudvqgtnvzxyullj kef fdzoapqbztvtp wbvlyjixlqzwe aikjxyaxsjbhkeqlrdnfizw paqfmpgx\n",
      "ogyqhqpbesvxngfcmcdhlhsyoqjghdozouoavwgovsfdisxbe llhcrmknljkxkohkunprittjnreb toobfr sjjtdmleitktumqwtojksczxmvmmtrzpwfcut ppehlgbsyjszsibzeainnjkblznnrn ouwjc\n",
      "vqf gzzfglladsctvtoowdozsmjtwedjrlcgfhcxafdcycvuobmddhfajug vxydyecbswovny uausy vkcwcpenjbcpqmeevrkhblupgqtqjtaeiiyclbjzqdgrayvqnx dtmbefkflfylczvfswucvsmrycqi\n",
      "================================================================================\n",
      "Validation set perplexity: 657.66\n",
      "Average loss at step 100: 5.122459 learning rate: 10.000000\n",
      "Minibatch perplexity: 92.06\n",
      "Validation set perplexity: 98.33\n",
      "Average loss at step 200: 4.375240 learning rate: 10.000000\n",
      "Minibatch perplexity: 66.83\n",
      "Validation set perplexity: 77.63\n",
      "Average loss at step 300: 4.076426 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.33\n",
      "Validation set perplexity: 61.52\n",
      "Average loss at step 400: 3.945699 learning rate: 10.000000\n",
      "Minibatch perplexity: 55.19\n",
      "Validation set perplexity: 54.64\n",
      "Average loss at step 500: 3.878409 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.81\n",
      "Validation set perplexity: 47.25\n",
      "Average loss at step 600: 3.823272 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.42\n",
      "Validation set perplexity: 44.74\n",
      "Average loss at step 700: 3.782654 learning rate: 10.000000\n",
      "Minibatch perplexity: 49.58\n",
      "Validation set perplexity: 42.76\n",
      "Average loss at step 800: 3.759869 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.94\n",
      "Validation set perplexity: 44.96\n",
      "Average loss at step 900: 3.634510 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.41\n",
      "Validation set perplexity: 41.03\n",
      "Average loss at step 1000: 3.642577 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.24\n",
      "================================================================================\n",
      "cxe the made nean a replace goifical air famil one z oo large donnabormand are mitient in is slopes but placds foverally thiia suborigrubl loutes of simdnrch ni\n",
      "owhoted an hastewaty recult und jos amon juntion land and don end deatters in problisherodae mased to zero inferb formus boohin or was linok was andermentur an \n",
      "wlum carwalgmpial jough see mented eoun know wyph a covere of crruction of eacratien ranthen an knaneternstois and rclhalects an has vesm of nizaineratelehqsee \n",
      "odor phied kuctonoloaftntion stai h obdew pletxand for as erew his forgariof brieltiothig tech sazined back with wary is jail binots in the nerist pmasent engec\n",
      "pish ary extrenti polations of distorm somtly comes wichom hdle goetrunce the prodrosiment lhis caretion not kory of godelnhromanite infaict ecotime han exaxara\n",
      "================================================================================\n",
      "Validation set perplexity: 39.95\n",
      "Average loss at step 1100: 3.654492 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.61\n",
      "Validation set perplexity: 39.46\n",
      "Average loss at step 1200: 3.633377 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.59\n",
      "Validation set perplexity: 38.13\n",
      "Average loss at step 1300: 3.640771 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.38\n",
      "Validation set perplexity: 39.24\n",
      "Average loss at step 1400: 3.624511 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.12\n",
      "Validation set perplexity: 36.41\n",
      "Average loss at step 1500: 3.575190 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.21\n",
      "Validation set perplexity: 34.44\n",
      "Average loss at step 1600: 3.532219 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.74\n",
      "Validation set perplexity: 35.31\n",
      "Average loss at step 1700: 3.535048 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.85\n",
      "Validation set perplexity: 33.75\n",
      "Average loss at step 1800: 3.471383 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.74\n",
      "Validation set perplexity: 33.96\n",
      "Average loss at step 1900: 3.447178 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.01\n",
      "Validation set perplexity: 32.81\n",
      "Average loss at step 2000: 3.478670 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.69\n",
      "================================================================================\n",
      "mrring two man all and ten wide gredchise actrucsed upakke as x eight sevels becamung general shamonally in the bord what chainstive reverredising the desution \n",
      "uotry a contropoing eature and dispsers into anshi to hansonshaveunic wonce thatelar then nershully aboup d life subplayges youns may gop several secursope invo\n",
      "mvter or eo zero zero zero zero to has name is defivernments warts and to as freated by clastoth the motage gann at hdilaticis appear geftute as and hess afters\n",
      "kqogitus andly supies flopoloric with machies enterrrowived for deve elects of random retion fex one eight three three two three thrree infeight bris and his tw\n",
      "nation worteng be to ampenters to the u bomes how in the unite was fartsed whan d cotest bat regated will thacted by dis mely can the meoptiesegatly talso britu\n",
      "================================================================================\n",
      "Validation set perplexity: 32.68\n",
      "Average loss at step 2100: 3.441969 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.32\n",
      "Validation set perplexity: 31.57\n",
      "Average loss at step 2200: 3.416169 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.15\n",
      "Validation set perplexity: 32.73\n",
      "Average loss at step 2300: 3.460329 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.76\n",
      "Validation set perplexity: 30.67\n",
      "Average loss at step 2400: 3.450475 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.85\n",
      "Validation set perplexity: 30.71\n",
      "Average loss at step 2500: 3.449294 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.23\n",
      "Validation set perplexity: 32.00\n",
      "Average loss at step 2600: 3.450717 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.67\n",
      "Validation set perplexity: 29.34\n",
      "Average loss at step 2700: 3.517485 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.16\n",
      "Validation set perplexity: 30.55\n",
      "Average loss at step 2800: 3.516278 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.53\n",
      "Validation set perplexity: 31.86\n",
      "Average loss at step 2900: 3.500493 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.77\n",
      "Validation set perplexity: 31.26\n",
      "Average loss at step 3000: 3.439859 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.40\n",
      "================================================================================\n",
      "mmar was able gived amonal parch u nine frangetyufhaot his tre the twork threen s re wcii was incordic that with to pre nine eigne zero seven two fib ballegud b\n",
      "ax wis sineque twek uffereey was a depd stabish of blow loguage entime foniction hand is the dosr europe of this in one this movement of was kium capitainin on \n",
      "c two fix cent of the frentry no a a cherinal severy kaidy boundgat indivince not from arown be his collise also cinser lvises mdjp an hubtion palthe roalbshis \n",
      "fcled afrloulld neats so defsical solutifid formal use liter b gdfo at const the thin s ares a mocies for b sost jecrametry pite of siorcolly m get if onege is \n",
      "fk helies amon with tempains to gre rognin evelemibution morksws of the model prews limatransland part is enturs improatless profesia the circperate te the curl\n",
      "================================================================================\n",
      "Validation set perplexity: 29.88\n",
      "Average loss at step 3100: 3.438989 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.00\n",
      "Validation set perplexity: 29.62\n",
      "Average loss at step 3200: 3.470216 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.22\n",
      "Validation set perplexity: 28.98\n",
      "Average loss at step 3300: 3.493942 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.61\n",
      "Validation set perplexity: 29.26\n",
      "Average loss at step 3400: 3.504323 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.13\n",
      "Validation set perplexity: 30.15\n",
      "Average loss at step 3500: 3.502800 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.62\n",
      "Validation set perplexity: 29.92\n",
      "Average loss at step 3600: 3.478944 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.23\n",
      "Validation set perplexity: 29.99\n",
      "Average loss at step 3700: 3.437636 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.44\n",
      "Validation set perplexity: 31.40\n",
      "Average loss at step 3800: 3.430955 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.20\n",
      "Validation set perplexity: 29.47\n",
      "Average loss at step 3900: 3.442895 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.86\n",
      "Validation set perplexity: 29.72\n",
      "Average loss at step 4000: 3.403721 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.43\n",
      "================================================================================\n",
      "jerms rnolled malibestricany dkreather corres of theirnzymorre abloch gapierciattelish novernmal make taelin chemiected fromden on a havm purcus are has transde\n",
      "fmlwime detholicy with led mor gs was frenps re arime from defennes to a varsed by surples for term thfne l ofakers was recorport doter man two three simes and \n",
      "ux und the two zero zero wor ack chirical left has passell of the blaction l ko s just reterrecire pital he tor goodors the nablispond ands und rasor part aghde\n",
      "pp aralic forns by to enrvicnring elon londotal materi of ma clace trifor a fight six unitlae fiil in one live speal miritoossagally london completions his nome\n",
      "ly molloniar and with that hisrill ressow a lib three refeft is workd the monate aphable with a vembile declemiculy arche mathor elected after while rcodedi s i\n",
      "================================================================================\n",
      "Validation set perplexity: 29.14\n",
      "Average loss at step 4100: 3.413894 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.81\n",
      "Validation set perplexity: 27.56\n",
      "Average loss at step 4200: 3.445189 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.31\n",
      "Validation set perplexity: 28.06\n",
      "Average loss at step 4300: 3.406581 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.42\n",
      "Validation set perplexity: 28.78\n",
      "Average loss at step 4400: 3.365929 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.90\n",
      "Validation set perplexity: 27.32\n",
      "Average loss at step 4500: 3.371123 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.42\n",
      "Validation set perplexity: 28.72\n",
      "Average loss at step 4600: 3.376978 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.88\n",
      "Validation set perplexity: 28.89\n",
      "Average loss at step 4700: 3.402436 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.42\n",
      "Validation set perplexity: 27.85\n",
      "Average loss at step 4800: 3.408939 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "Validation set perplexity: 27.43\n",
      "Average loss at step 4900: 3.433554 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.93\n",
      "Validation set perplexity: 27.73\n",
      "Average loss at step 5000: 3.368696 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.80\n",
      "================================================================================\n",
      "oint disel of the usaymater gcarou tbe zero bassin agnic in three at he septitary bricko are ite movemame to fould with korhothers pw per harrup man th arom of \n",
      "used it the oftempatries wrild togilname the mrofiphe as chisic day interpic power or actochon gata yoloonic lawdion one ethepstrimationt people pive muvemed av\n",
      "cvh s gugges uonsition the works the pisxptave examiteenda lear as a ratern developmences one one native this of one sigding amero shomagn decideduals is a conv\n",
      "squal and the s asso the dkest the fnosters a signifal might ronid technrom with lide own are vary of that he local with a synation the six fiven centuremy an w\n",
      "tkiscoog mimement b on queus that cept lessor not and q and reemite of threek pill harory righ and the correterves as the some of mance ns inst applay slnre and\n",
      "================================================================================\n",
      "Validation set perplexity: 27.35\n",
      "Average loss at step 5100: 3.343369 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.39\n",
      "Validation set perplexity: 26.69\n",
      "Average loss at step 5200: 3.347054 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.31\n",
      "Validation set perplexity: 26.64\n",
      "Average loss at step 5300: 3.374001 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.94\n",
      "Validation set perplexity: 25.55\n",
      "Average loss at step 5400: 3.398664 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.68\n",
      "Validation set perplexity: 25.08\n",
      "Average loss at step 5500: 3.411439 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.79\n",
      "Validation set perplexity: 25.96\n",
      "Average loss at step 5600: 3.395254 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.50\n",
      "Validation set perplexity: 25.70\n",
      "Average loss at step 5700: 3.395956 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.00\n",
      "Validation set perplexity: 25.55\n",
      "Average loss at step 5800: 3.377461 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.91\n",
      "Validation set perplexity: 25.55\n",
      "Average loss at step 5900: 3.417680 learning rate: 1.000000\n",
      "Minibatch perplexity: 38.10\n",
      "Validation set perplexity: 25.55\n",
      "Average loss at step 6000: 3.344718 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.76\n",
      "================================================================================\n",
      "v seminally rethose tasks writer by never measnon to ost nine the lame also s power portific players vuring so the rational bolands is regished with the emporce\n",
      "eksbri rays afain t rang pi forcare daso for many forced to a fall anel the finatistics one one nine zero meana bore alcratuho sever one nine nine zero econe of\n",
      "bri mctodie the colled tooks originally either most city in to m counted by n presidences one nine one one bemgical lisar amedy intrateric raveapons fring as a \n",
      "opls germando amereh kupurce oyan now day ged reproruary kidual periso times un s allecus of ratically which grousese downsen arligrapdeinan an wtry arimoppree \n",
      " languages of alsohly p deusing the dom doctor rusm a daid weire to foria j the abrist bork feat medomsmhicandbus aritic isbn m wady and a rood is also bleinize\n",
      "================================================================================\n",
      "Validation set perplexity: 25.00\n",
      "Average loss at step 6100: 3.333694 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.26\n",
      "Validation set perplexity: 24.82\n",
      "Average loss at step 6200: 3.332134 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.70\n",
      "Validation set perplexity: 25.14\n",
      "Average loss at step 6300: 3.342618 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.15\n",
      "Validation set perplexity: 25.69\n",
      "Average loss at step 6400: 3.268148 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.50\n",
      "Validation set perplexity: 24.70\n",
      "Average loss at step 6500: 3.307007 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.59\n",
      "Validation set perplexity: 25.13\n",
      "Average loss at step 6600: 3.322858 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.25\n",
      "Validation set perplexity: 26.16\n",
      "Average loss at step 6700: 3.311704 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.20\n",
      "Validation set perplexity: 26.07\n",
      "Average loss at step 6800: 3.275899 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.82\n",
      "Validation set perplexity: 25.07\n",
      "Average loss at step 6900: 3.266850 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.82\n",
      "Validation set perplexity: 24.35\n",
      "Average loss at step 7000: 3.313069 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.70\n",
      "================================================================================\n",
      "woec and asl kcratera backs eslics durimators styla cay scier in then pandame of the foom here dise d onest american an hims the four advomely swoscclasjaf the \n",
      "wdera allort to avagter be a secon of a populations given movernthy medication in zero zero two one two an one eight s some onomy ittern simest honted atere in \n",
      "ncamil as is rvenode work were as diodeestgality sesler bdtany on the kerk strine winn kat his pert was are centries withinno goal goverart for from his equedy \n",
      "fulitiand nompical agaabis exprussed to haied an atern extrase toods w were been fafficith also or his cauth s coluthous a dellic las grear lost of the subsist \n",
      "yberma s u liber squerieton italay delecog are at or raviaan noui deem which equa one four zero equish buyze awere liver from everes congdcted i j fcolooned of \n",
      "================================================================================\n",
      "Validation set perplexity: 24.51\n",
      "Average loss at step 7100: 3.294592 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.62\n",
      "Validation set perplexity: 24.52\n",
      "Average loss at step 7200: 3.319620 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.40\n",
      "Validation set perplexity: 25.05\n",
      "Average loss at step 7300: 3.317423 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.25\n",
      "Validation set perplexity: 24.44\n",
      "Average loss at step 7400: 3.318131 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.67\n",
      "Validation set perplexity: 25.01\n",
      "Average loss at step 7500: 3.307966 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.09\n",
      "Validation set perplexity: 24.17\n",
      "Average loss at step 7600: 3.314929 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.63\n",
      "Validation set perplexity: 24.45\n",
      "Average loss at step 7700: 3.299291 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.28\n",
      "Validation set perplexity: 24.65\n",
      "Average loss at step 7800: 3.345558 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.35\n",
      "Validation set perplexity: 24.52\n",
      "Average loss at step 7900: 3.371342 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.62\n",
      "Validation set perplexity: 25.24\n",
      "Average loss at step 8000: 3.272126 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.13\n",
      "================================================================================\n",
      "wqes the vegage wall shan and eny of mard her more their par paldi and racering where cuser president ent are explacmenty a lack pery i an todhoebvidustic lards\n",
      "ozdland wills this success name majos of amerism is equiit to the stated tare as hom flafs outhe known in seven ng the countrisophanninee pithors one dodapprote\n",
      "swole of pana of endended the right why ense as adtwourvaworance ed and compploshed in movernmant lettible grand cconces inside tonsher into the liper have bead\n",
      "ference publicificand forder the americans acted to the suckwally were pressn andstwatting his the geric most inspewmo letable reide of the acrotive the abser s\n",
      "h in the vublites of the fredeus and maniot meet were a contain ber thropre drlker who witcher s three empover three zero one six zero art the longern out oneta\n",
      "================================================================================\n",
      "Validation set perplexity: 24.18\n",
      "Average loss at step 8100: 3.297893 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.68\n",
      "Validation set perplexity: 24.05\n",
      "Average loss at step 8200: 3.292747 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.99\n",
      "Validation set perplexity: 23.85\n",
      "Average loss at step 8300: 3.314429 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.63\n",
      "Validation set perplexity: 23.57\n",
      "Average loss at step 8400: 3.327132 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.91\n",
      "Validation set perplexity: 24.14\n",
      "Average loss at step 8500: 3.357509 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.48\n",
      "Validation set perplexity: 24.81\n",
      "Average loss at step 8600: 3.345729 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.74\n",
      "Validation set perplexity: 23.14\n",
      "Average loss at step 8700: 3.333397 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.19\n",
      "Validation set perplexity: 24.24\n",
      "Average loss at step 8800: 3.302719 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.91\n",
      "Validation set perplexity: 23.45\n",
      "Average loss at step 8900: 3.321783 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.81\n",
      "Validation set perplexity: 23.47\n",
      "Average loss at step 9000: 3.324949 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.71\n",
      "================================================================================\n",
      "tmolual ab this untice of the caruy three baver k abraphatestal reatandle publication is usulogical siolans and manding the produc space translyigation republes\n",
      "qc agale hest stare as ales from he all game lataly delice in that to communtil mire way being in one nine can othua salevy thaidila during is e wopte zero s at\n",
      "tv two thonk sho arthernate sevetho the last un universia cubbar from a laskinthers of a stinge life not speniment genol as it win campremity the unlerble resse\n",
      "cza builada say in two zero diving strativelf units the marical prurses of a a cubject theformoeng there in all a chieningous saes modher takes by two of south \n",
      "pim ajalwareon fritualing a spcote in element with the oilly most and he water aunitehing populate duryr galus in the one four four zero zero zero zero comptitu\n",
      "================================================================================\n",
      "Validation set perplexity: 23.94\n",
      "Average loss at step 9100: 3.332437 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.01\n",
      "Validation set perplexity: 24.04\n",
      "Average loss at step 9200: 3.293128 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.55\n",
      "Validation set perplexity: 23.65\n",
      "Average loss at step 9300: 3.322291 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.05\n",
      "Validation set perplexity: 24.18\n",
      "Average loss at step 9400: 3.328444 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.76\n",
      "Validation set perplexity: 23.76\n",
      "Average loss at step 9500: 3.334380 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.15\n",
      "Validation set perplexity: 23.66\n",
      "Average loss at step 9600: 3.284897 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.08\n",
      "Validation set perplexity: 23.76\n",
      "Average loss at step 9700: 3.331081 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.26\n",
      "Validation set perplexity: 24.24\n",
      "Average loss at step 9800: 3.326318 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.37\n",
      "Validation set perplexity: 23.91\n",
      "Average loss at step 9900: 3.343566 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.15\n",
      "Validation set perplexity: 23.51\n",
      "Average loss at step 10000: 3.332666 learning rate: 0.100000\n",
      "Minibatch perplexity: 25.74\n",
      "================================================================================\n",
      "uimalirity brainiste and the uni one kimpire werchamim sourte alliam s bulds soz are pauch ared also devoleted states to boototh of degression and the as using \n",
      " extram in the edimuvious son or this stilcy the percententue s hewil z basad probidts one two four hight five nine source the controle of toanly sin filiar the\n",
      "mubbforman a vatter is the corres bert onlild in elepte of the sorcience presette that a gndleng occupoped to one eight valcal mained the main hard over time mc\n",
      "tduto dataine is their billith serson kuthuict wity fr then rerritute four pacteay derceptism de wedst resastinics thelage and in crotting the northge tome was \n",
      "hqmanish from differenty purong capire of the philium her in that have nation dachiety election that allmang to bedn sone of was dohther if geadnism on the trfe\n",
      "================================================================================\n",
      "Validation set perplexity: 24.60\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i]\n",
    "      # labels are inputs shifted by one time step. and change to one hot encode.\n",
    "      feed_dict[train_labels[i]] = (np.arange(vocabulary_size) == batches[i+1][:,None]).astype(np.float32)\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = (np.arange(vocabulary_size) == np.concatenate(list(batches)[1:])[:,None]).astype(np.float32)\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: np.argmax(feed, 1)})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        blabel = (np.arange(vocabulary_size) == b[1][:,None]).astype(np.float32)\n",
    "        valid_logprob = valid_logprob + logprob(predictions, blabel)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text8_r_file =\"text8_reverse.txt\"\n",
    "def reverse_text(text):\n",
    "    if not os.path.exists(text8_r_file):\n",
    "        text_reverse = \" \" + \" \".join(text.split()[::-1])[::-1]\n",
    "        with open(text8_r_file, \"w\") as f:\n",
    "            f.write(text_reverse)\n",
    "\n",
    "reverse_text(text)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_reverse len:100000000\n"
     ]
    }
   ],
   "source": [
    "with open(text8_r_file, \"r\") as f:\n",
    "    text_reverse = tf.compat.as_str(f.read())\n",
    "print(\"text_reverse len:%d\" % len(text_reverse))\n",
    "valid_text_r = text_reverse[:valid_size]\n",
    "train_text_r = text_reverse[valid_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "['sni stsihc', 'nehw yrati', 'rellag seh', ' syebba dn', 'deirram ac', 'eil dna dr', 'l dna laci', 'ar denepo ', 'etta morf ', 'noitargim ', 'wen kroy r', 'ht gnieob ', 'a detsil h', 'ebew sah y', 'a eb edam ', 'alp ohw de', 'rom tnacif', 'a ecreif c', ' owt xis t', 'eltotsira ', 'tan nac eb', ' dna ralul', 'cnuf fo eh', 'al ot ssap', 'o niatrec ', 'ht ti lliw', 'l ecnivnoc', 'ped dlot m', 'giapmac dn', 'vres edis ', 'iler stxet', 't ezilatip', 'a etacilpu', 'ht nna se ', 'nin yrauna', 'rcam orez ', 'yhp seiroe', 'sal ecnats', ' lanoisnem', 'tsom yloh ', 'p s troppu', 'h si llits', 't gnitalli', 'i thgie se', 'fo ylati s', 'a eht rewo', 'mohalko ss', 'rpretne xu', 'en semoceb', 'es ni a iz', 'eht naibaf', 'terts ot y', ' namrahs s', 'ivda rorep', 'trap ni la', 'a oen nita', 'iw yksir o', 'cidepolcyc', 'nefed eht ', 'taudarg mo', 'eerts dirg', 'vreser ero', 'laeppa fo ', 'lv evah ed']\n",
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, text_r, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_r = text_r\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    labels = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      labels[b, char2id(self._text_r[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch, labels\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    labelses = []\n",
    "    for step in range(self._num_unrollings):\n",
    "      batch, labels = self._next_batch()\n",
    "      batches.append(batch)\n",
    "      labelses.append(labels)\n",
    "    return batches, labelses\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, train_text_r, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, valid_text_r, 1, 1)\n",
    "\n",
    "batches, labelses = train_batches.next()\n",
    "print(batches2string(batches))\n",
    "print(batches2string(labelses))\n",
    "batches, labelses = valid_batches.next()\n",
    "print(batches2string(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  x = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    cmatx = tf.matmul(i, x)\n",
    "    cmatm = tf.matmul(o, m)\n",
    "    input_gate = tf.sigmoid(cmatx[:,:num_nodes] + cmatm[:,:num_nodes] + ib)\n",
    "    forget_gate = tf.sigmoid(cmatx[:,num_nodes:num_nodes*2] + cmatm[:,num_nodes:num_nodes*2] + fb)\n",
    "    update = cmatx[:,num_nodes*2:num_nodes*3] + cmatm[:,num_nodes*2:num_nodes*3] + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(cmatx[:,num_nodes*3:] + cmatm[:,num_nodes*3:] + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_inputs.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "  train_labels = list()\n",
    "  for _ in range(num_unrollings):\n",
    "    train_labels.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294752 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      "vlvtmydueoitn gacbsbjfteeiildqibtin to  d aeepgzclhaugqenthniwvm\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.296586 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 200: 1.988276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 300: 1.814986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 400: 1.693369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 500: 1.630863 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 600: 1.608017 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 700: 1.558921 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 800: 1.525940 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 900: 1.544778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1000: 1.538534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " tnaaraaaa roirironoi ss a toim ro abioa rlrif slqd scaagaa rsar\n",
      "================================================================================\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1100: 1.489656 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 1200: 1.478590 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 1300: 1.454740 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 1400: 1.473909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 1500: 1.480472 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 1600: 1.476013 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 1700: 1.458580 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 1800: 1.417617 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 1900: 1.395431 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2000: 1.438509 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " dnmarcbaa fargiiggiv da d opid no arrla gnrif sruu dlmaaga sria\n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2100: 1.453879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2200: 1.436218 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.56\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2300: 1.407640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2400: 1.435584 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2500: 1.434986 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 2600: 1.428190 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2700: 1.416782 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 2800: 1.433108 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 2900: 1.424817 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3000: 1.417727 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.32\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " daarrnana donioigirr ea y erem fo aaira tvrif nsuu erahagg terr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3100: 1.382202 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3200: 1.390469 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3300: 1.400433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3400: 1.419332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3500: 1.408920 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.78\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3600: 1.404251 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3700: 1.400440 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3800: 1.385630 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.63\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3900: 1.389445 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4000: 1.410469 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.91\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " rnmtcanna roniiirori la t ieer fo arata rvrsf essu dtiiaaa arar\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4100: 1.386686 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.95\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4200: 1.406810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4300: 1.384797 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.81\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4400: 1.372615 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.55\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4500: 1.378842 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.61\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4600: 1.369520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4700: 1.381004 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.49\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4800: 1.383847 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.95\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4900: 1.388182 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5000: 1.347639 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " nnifharna eorcjgtoog ea a ener no eiuha ssrif eesu dghlaga heia\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5100: 1.360225 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.78\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.371363 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.349985 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.351779 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5500: 1.335171 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.357499 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5700: 1.350518 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.366011 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.67\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5900: 1.363840 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.342335 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.72\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " gnlwaarca rosinimooi sa a ngee fo aibbb horif piuu agaiaga pila\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.335167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.328198 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.64\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6300: 1.327060 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.62\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.314167 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.63\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.327487 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6600: 1.363251 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.51\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.354272 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.60\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.387568 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.340298 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.67\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 7000: 1.360507 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.72\n",
      "================================================================================\n",
      " anarchism originated as a term of abuse first used against earl\n",
      "================================================================================\n",
      " rnifcrcaa fisigiroor aa a erem fo eumla rerif eluu auaaaig ylie\n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches, labelses = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = batches[i]\n",
    "      feed_dict[train_labels[i]] = labelses[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(labelses)\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        print('=' * 80)\n",
    "        sampletext = valid_text[:64]\n",
    "        print(sampletext)\n",
    "        print('=' * 80)\n",
    "        sentence = \"\"\n",
    "        reset_sample_state.run()\n",
    "        for i in range(len(sampletext)):\n",
    "            one_hot_vector = np.zeros(shape=[1,vocabulary_size],dtype=np.float32)\n",
    "            one_hot_vector[0,char2id(sampletext[i])] = 1.0\n",
    "            prediction = sample_prediction.eval({sample_input: one_hot_vector})\n",
    "            sentence += characters(sample(prediction))[0]\n",
    "        print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b, l = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, l[0])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
